{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569df2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97e608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df = pd.read_csv(\"data/bank-full.csv\", sep = \";\")\n",
    "data_train = pd.read_csv(\"data/train.csv\")\n",
    "data_train = data_train.drop([\"id\"], axis =1)\n",
    "# Convert 'y' column in original_df from 'yes'/'no' to 1/0\n",
    "# original_df['y'] = original_df['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Concatenate original_df and df (ignore index to avoid duplicate indices)\n",
    "# data_train = pd.concat([original_df, df], ignore_index=True)\n",
    "\n",
    "data_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2385c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 18:34:41,269] A new study created in memory with name: no-name-125e9e65-c79f-409b-b5fd-dcf9fa2b4449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 18:35:43,009] Trial 0 finished with value: 0.9669207500679496 and parameters: {'learning_rate': 0.09912739347765478, 'num_leaves': 83, 'feature_fraction': 0.7111258957046671, 'bagging_fraction': 0.9100507208319891, 'bagging_freq': 9, 'min_data_in_leaf': 60, 'lambda_l1': 0.7753613270478349, 'lambda_l2': 4.887088407557567}. Best is trial 0 with value: 0.9669207500679496.\n",
      "[I 2025-08-15 18:36:39,250] Trial 1 finished with value: 0.9660154898591515 and parameters: {'learning_rate': 0.052676881221803445, 'num_leaves': 220, 'feature_fraction': 0.8400690302051211, 'bagging_fraction': 0.6414923243486647, 'bagging_freq': 1, 'min_data_in_leaf': 176, 'lambda_l1': 0.06928376912448297, 'lambda_l2': 2.646643961113116}. Best is trial 0 with value: 0.9669207500679496.\n",
      "[I 2025-08-15 18:37:27,109] Trial 2 finished with value: 0.9662978143181139 and parameters: {'learning_rate': 0.13152399972430873, 'num_leaves': 43, 'feature_fraction': 0.7079593369456566, 'bagging_fraction': 0.7541972392329759, 'bagging_freq': 6, 'min_data_in_leaf': 134, 'lambda_l1': 0.5057895444844612, 'lambda_l2': 1.3267031740569524}. Best is trial 0 with value: 0.9669207500679496.\n",
      "[I 2025-08-15 18:38:28,040] Trial 3 finished with value: 0.9675865028918476 and parameters: {'learning_rate': 0.1340750725613897, 'num_leaves': 167, 'feature_fraction': 0.7446673442699354, 'bagging_fraction': 0.6568184379872212, 'bagging_freq': 6, 'min_data_in_leaf': 37, 'lambda_l1': 2.895081691820494, 'lambda_l2': 3.041018282451339}. Best is trial 3 with value: 0.9675865028918476.\n",
      "[I 2025-08-15 18:39:26,030] Trial 4 finished with value: 0.9677515397713785 and parameters: {'learning_rate': 0.13414104600862187, 'num_leaves': 106, 'feature_fraction': 0.9762140923853682, 'bagging_fraction': 0.9699571051340866, 'bagging_freq': 5, 'min_data_in_leaf': 23, 'lambda_l1': 3.6051388516470535, 'lambda_l2': 0.5485687645589565}. Best is trial 4 with value: 0.9677515397713785.\n",
      "[I 2025-08-15 18:40:31,366] Trial 5 finished with value: 0.9655547601094403 and parameters: {'learning_rate': 0.0485138250072253, 'num_leaves': 197, 'feature_fraction': 0.8871622594755533, 'bagging_fraction': 0.7698949276338769, 'bagging_freq': 8, 'min_data_in_leaf': 138, 'lambda_l1': 4.8783042456233146, 'lambda_l2': 1.2085040861220149}. Best is trial 4 with value: 0.9677515397713785.\n",
      "[I 2025-08-15 18:41:33,578] Trial 6 finished with value: 0.9673901462052279 and parameters: {'learning_rate': 0.09738621480322861, 'num_leaves': 135, 'feature_fraction': 0.8871509056186395, 'bagging_fraction': 0.7970241589020705, 'bagging_freq': 9, 'min_data_in_leaf': 140, 'lambda_l1': 0.16145573659654355, 'lambda_l2': 3.765786988912267}. Best is trial 4 with value: 0.9677515397713785.\n",
      "[I 2025-08-15 18:42:42,376] Trial 7 finished with value: 0.9648446735283047 and parameters: {'learning_rate': 0.036627245340867844, 'num_leaves': 224, 'feature_fraction': 0.8136032376439913, 'bagging_fraction': 0.7795177665376938, 'bagging_freq': 8, 'min_data_in_leaf': 152, 'lambda_l1': 0.8943103398249924, 'lambda_l2': 1.7204039615539006}. Best is trial 4 with value: 0.9677515397713785.\n",
      "[I 2025-08-15 18:43:46,829] Trial 8 finished with value: 0.9678242078178707 and parameters: {'learning_rate': 0.15525120900779693, 'num_leaves': 200, 'feature_fraction': 0.9217238548419453, 'bagging_fraction': 0.8902111839004927, 'bagging_freq': 4, 'min_data_in_leaf': 60, 'lambda_l1': 0.4885866247164855, 'lambda_l2': 2.584327603722376}. Best is trial 8 with value: 0.9678242078178707.\n",
      "[I 2025-08-15 18:44:54,173] Trial 9 finished with value: 0.9677253004277268 and parameters: {'learning_rate': 0.1919181139760581, 'num_leaves': 251, 'feature_fraction': 0.9453967427449044, 'bagging_fraction': 0.9114756862225618, 'bagging_freq': 3, 'min_data_in_leaf': 86, 'lambda_l1': 0.08035793766727917, 'lambda_l2': 3.853887595207462}. Best is trial 8 with value: 0.9678242078178707.\n",
      "[I 2025-08-15 18:45:55,320] Trial 10 finished with value: 0.9678142659083412 and parameters: {'learning_rate': 0.1766802440493908, 'num_leaves': 179, 'feature_fraction': 0.9993328390587107, 'bagging_fraction': 0.8805803721099603, 'bagging_freq': 3, 'min_data_in_leaf': 86, 'lambda_l1': 1.9690648868747993, 'lambda_l2': 0.12229547016998943}. Best is trial 8 with value: 0.9678242078178707.\n",
      "[I 2025-08-15 18:46:55,955] Trial 11 finished with value: 0.9676627741971888 and parameters: {'learning_rate': 0.18461919054300063, 'num_leaves': 176, 'feature_fraction': 0.9893639585217944, 'bagging_fraction': 0.8761016058070508, 'bagging_freq': 3, 'min_data_in_leaf': 94, 'lambda_l1': 1.6089585972972102, 'lambda_l2': 0.1701444512643886}. Best is trial 8 with value: 0.9678242078178707.\n",
      "[I 2025-08-15 18:47:56,113] Trial 12 finished with value: 0.96780576847773 and parameters: {'learning_rate': 0.16718032135175487, 'num_leaves': 147, 'feature_fraction': 0.9172785614835721, 'bagging_fraction': 0.8610721259712213, 'bagging_freq': 3, 'min_data_in_leaf': 72, 'lambda_l1': 1.8154276197325525, 'lambda_l2': 1.9484483032881275}. Best is trial 8 with value: 0.9678242078178707.\n",
      "[I 2025-08-15 18:48:46,025] Trial 13 finished with value: 0.9681533877622965 and parameters: {'learning_rate': 0.15925965653801102, 'num_leaves': 197, 'feature_fraction': 0.6169584950457618, 'bagging_fraction': 0.9938556573875051, 'bagging_freq': 1, 'min_data_in_leaf': 53, 'lambda_l1': 1.8757251077314805, 'lambda_l2': 3.2335164881539975}. Best is trial 13 with value: 0.9681533877622965.\n",
      "[I 2025-08-15 18:49:38,297] Trial 14 finished with value: 0.9681711872510683 and parameters: {'learning_rate': 0.1519493889104291, 'num_leaves': 249, 'feature_fraction': 0.6156816286021847, 'bagging_fraction': 0.9874631266778287, 'bagging_freq': 1, 'min_data_in_leaf': 51, 'lambda_l1': 2.8476275475429267, 'lambda_l2': 3.398872546748853}. Best is trial 14 with value: 0.9681711872510683.\n",
      "[I 2025-08-15 18:50:32,424] Trial 15 finished with value: 0.9673312225548955 and parameters: {'learning_rate': 0.07442711614843439, 'num_leaves': 242, 'feature_fraction': 0.6006467539565777, 'bagging_fraction': 0.9740875504396885, 'bagging_freq': 1, 'min_data_in_leaf': 42, 'lambda_l1': 2.995835107457838, 'lambda_l2': 3.5901419262158374}. Best is trial 14 with value: 0.9681711872510683.\n",
      "[I 2025-08-15 18:51:26,386] Trial 16 finished with value: 0.9682553723540973 and parameters: {'learning_rate': 0.15634817117829355, 'num_leaves': 254, 'feature_fraction': 0.6022437295197862, 'bagging_fraction': 0.9906312656080055, 'bagging_freq': 1, 'min_data_in_leaf': 114, 'lambda_l1': 3.9272928824388362, 'lambda_l2': 4.944056334075543}. Best is trial 16 with value: 0.9682553723540973.\n",
      "[I 2025-08-15 18:52:41,419] Trial 17 finished with value: 0.96178141286482 and parameters: {'learning_rate': 0.013195637188254533, 'num_leaves': 255, 'feature_fraction': 0.6643295847157331, 'bagging_fraction': 0.9572464818479024, 'bagging_freq': 2, 'min_data_in_leaf': 110, 'lambda_l1': 4.283911591058856, 'lambda_l2': 4.836143847187198}. Best is trial 16 with value: 0.9682553723540973.\n",
      "[I 2025-08-15 18:53:55,832] Trial 18 finished with value: 0.9680143985977606 and parameters: {'learning_rate': 0.1183500862063239, 'num_leaves': 229, 'feature_fraction': 0.6520136866952235, 'bagging_fraction': 0.8281023347757209, 'bagging_freq': 2, 'min_data_in_leaf': 111, 'lambda_l1': 3.795082959563863, 'lambda_l2': 4.377924458979207}. Best is trial 16 with value: 0.9682553723540973.\n",
      "[I 2025-08-15 18:54:40,020] Trial 19 finished with value: 0.9662323550514621 and parameters: {'learning_rate': 0.143186459180099, 'num_leaves': 37, 'feature_fraction': 0.7729842230268539, 'bagging_fraction': 0.7060130672307672, 'bagging_freq': 5, 'min_data_in_leaf': 195, 'lambda_l1': 3.4888126146382232, 'lambda_l2': 4.4827345916588595}. Best is trial 16 with value: 0.9682553723540973.\n",
      "[I 2025-08-15 18:54:40,021] A new study created in memory with name: no-name-eec12192-c1b6-4f5a-8811-6fab1f15f6b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:54:40] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:54:40] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:54:53] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:54:53] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:07] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:07] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:21] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:21] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:38] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:38] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "[I 2025-08-15 18:55:50,226] Trial 0 finished with value: 0.9483000061070357 and parameters: {'learning_rate': 0.19200683258044454, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.8001167705888174, 'colsample_bytree': 0.6465925992112143, 'lambda': 3.487939475083908, 'alpha': 0.7223907192122914}. Best is trial 0 with value: 0.9483000061070357.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:50] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:55:50] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:57:19] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:57:19] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:58:28] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:58:28] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:59:58] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [18:59:58] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:01:33] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:01:33] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "[I 2025-08-15 19:02:52,806] Trial 1 finished with value: 0.9488276917153918 and parameters: {'learning_rate': 0.12747162206511495, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.9626178889119366, 'colsample_bytree': 0.6814344953374966, 'lambda': 1.6785250512356358, 'alpha': 3.131931591485018}. Best is trial 1 with value: 0.9488276917153918.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:02:52] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:02:52] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:04:23] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:04:23] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:05:31] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:05:31] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:06:56] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:06:56] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:08:33] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:08:33] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "[I 2025-08-15 19:09:44,565] Trial 2 finished with value: 0.9484393628865843 and parameters: {'learning_rate': 0.10264908629024529, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.608888581478677, 'colsample_bytree': 0.6857611669978161, 'lambda': 0.737322595673548, 'alpha': 1.5447786471861917}. Best is trial 1 with value: 0.9488276917153918.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:09:44] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:09:44] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:11:17] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [19:11:17] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "[W 2025-08-15 19:11:55,811] Trial 3 failed with parameters: {'learning_rate': 0.1034821458716693, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7711146230465281, 'colsample_bytree': 0.835031660746738, 'lambda': 3.3846866822935224, 'alpha': 0.28867782003810016} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5368/1987616234.py\", line 97, in tune_xgb\n",
      "    model = xgb.train(params, dtrain, num_boost_round=5000,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/core.py\", line 2353, in eval_set\n",
      "    _LIB.XGBoosterEvalOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-15 19:11:55,817] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTuning XGBoost...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m study_xgb = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[43mstudy_xgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtune_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTuning CatBoost...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    137\u001b[39m study_cat = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mtune_xgb\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     95\u001b[39m     dtrain = xgb.DMatrix(X.iloc[train_idx][xgb_features], label=y.iloc[train_idx])\n\u001b[32m     96\u001b[39m     dvalid = xgb.DMatrix(X.iloc[valid_idx][xgb_features], label=y.iloc[valid_idx])\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     model = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     oof[valid_idx] = model.predict(dvalid, iteration_range=(\u001b[32m0\u001b[39m, model.best_iteration))\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc_score(y, oof)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/training.py:184\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    183\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    187\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/callback.py:264\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/core.py:2353\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2350\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   2351\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   2352\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2353\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterEvalOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2361\u001b[39m )\n\u001b[32m   2362\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2363\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Data from Step 2\n",
    "# ================================\n",
    "train_f = data_train.copy()\n",
    "test_f = data_test.copy()\n",
    "\n",
    "TARGET_COL = \"y\"\n",
    "id_col = \"id\"\n",
    "train_f[TARGET_COL] = train_f[TARGET_COL].astype(int)\n",
    "\n",
    "# 2) Cast ALL object columns to 'category' and align categories across train/test\n",
    "all_obj_cols = [c for c in train_f.columns if train_f[c].dtype == \"O\"]\n",
    "for c in all_obj_cols:\n",
    "    train_f[c] = train_f[c].astype(\"category\")\n",
    "    if c in test_f.columns:\n",
    "        test_f[c] = test_f[c].astype(\"category\")\n",
    "\n",
    "# Also align categories for any pre-existing 'category' dtype columns (e.g., campaign_bins)\n",
    "all_cat_cols = [c for c in train_f.columns if str(train_f[c].dtype).startswith(\"category\")]\n",
    "for c in all_cat_cols:\n",
    "    if c in test_f.columns:\n",
    "        cats = sorted(list(set(train_f[c].cat.categories.tolist()) |\n",
    "                           set(test_f[c].cat.categories.tolist())))\n",
    "        train_f[c] = train_f[c].cat.set_categories(cats)\n",
    "        test_f[c]  = test_f[c].cat.set_categories(cats)\n",
    "\n",
    "# 3) Build feature matrices\n",
    "TARGET_COL = \"y\"\n",
    "features = [c for c in train_f.columns if c not in {TARGET_COL, id_col}]\n",
    "X = train_f[features].copy()\n",
    "y = train_f[TARGET_COL].copy()\n",
    "X_test = test_f[features].copy()\n",
    "\n",
    "# For LightGBM / CatBoost: list of categorical feature names + indices\n",
    "cat_cols = [c for c in X.columns if str(X[c].dtype).startswith(\"category\")]\n",
    "cat_idx  = [X.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# For XGBoost: ONLY numeric/boolean columns\n",
    "xgb_features = [c for c in X.columns if not str(X[c].dtype).startswith(\"category\")]\n",
    "# (LightGBM error stemmed from object dtypes; now fixed by casting to category)\n",
    "\n",
    "# ================================\n",
    "# CV setup\n",
    "# ================================\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ================================\n",
    "# Model tuning functions\n",
    "# ================================\n",
    "def tune_lgb(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0),\n",
    "        \"verbose\": -1,\n",
    "        \"is_unbalance\": True,\n",
    "        \"seed\": RANDOM_STATE\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        dtrain = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx], categorical_feature=cat_cols)\n",
    "        dvalid = lgb.Dataset(X.iloc[valid_idx], y.iloc[valid_idx], categorical_feature=cat_cols)\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dvalid],\n",
    "                          callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        oof[valid_idx] = model.predict(X.iloc[valid_idx], num_iteration=model.best_iteration)\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "def tune_xgb(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",  # GPU if available\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.0, 5.0),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\n",
    "        \"scale_pos_weight\": 1.0\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        dtrain = xgb.DMatrix(X.iloc[train_idx][xgb_features], label=y.iloc[train_idx])\n",
    "        dvalid = xgb.DMatrix(X.iloc[valid_idx][xgb_features], label=y.iloc[valid_idx])\n",
    "        model = xgb.train(params, dtrain, num_boost_round=5000,\n",
    "                          evals=[(dvalid, \"valid\")], early_stopping_rounds=100,\n",
    "                          verbose_eval=False)\n",
    "        oof[valid_idx] = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "def tune_cat(trial):\n",
    "    params = {\n",
    "        \"iterations\": 5000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"bootstrap_type\": \"Bernoulli\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"task_type\": \"GPU\",  # or \"CPU\"\n",
    "        \"random_seed\": RANDOM_STATE,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for train_idx, valid_idx in skf.split(X, y):\n",
    "        train_pool = Pool(X.iloc[train_idx], y.iloc[train_idx], cat_features=cat_cols)\n",
    "        valid_pool = Pool(X.iloc[valid_idx], y.iloc[valid_idx], cat_features=cat_cols)\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100, verbose=False)\n",
    "        oof[valid_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "# ================================\n",
    "# Run tuning (small trials for demo)\n",
    "# ================================\n",
    "print(\"Tuning LightGBM...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "study_lgb.optimize(tune_lgb, n_trials=20)\n",
    "\n",
    "print(\"Tuning XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "study_xgb.optimize(tune_xgb, n_trials=20)\n",
    "\n",
    "print(\"Tuning CatBoost...\")\n",
    "study_cat = optuna.create_study(direction=\"maximize\")\n",
    "study_cat.optimize(tune_cat, n_trials=20)\n",
    "\n",
    "# ================================\n",
    "# Train final models with best params and blend\n",
    "# ================================\n",
    "def train_oof_preds():\n",
    "    preds_test_all = []\n",
    "    oof_all = []\n",
    "    for name, params, train_fn in [\n",
    "        (\"lgb\", study_lgb.best_params, \"lightgbm\"),\n",
    "        (\"xgb\", study_xgb.best_params, \"xgboost\"),\n",
    "        (\"cat\", study_cat.best_params, \"catboost\")\n",
    "    ]:\n",
    "        oof = np.zeros(len(X))\n",
    "        preds_test = np.zeros(len(X_test))\n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            if train_fn == \"lightgbm\":\n",
    "                dtrain = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx], categorical_feature=cat_cols)\n",
    "                dvalid = lgb.Dataset(X.iloc[valid_idx], y.iloc[valid_idx], categorical_feature=cat_cols)\n",
    "                model = lgb.train({**params, \"objective\": \"binary\", \"metric\": \"auc\", \"verbose\": -1},\n",
    "                                  dtrain, valid_sets=[dvalid],\n",
    "                                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "                oof[valid_idx] = model.predict(X.iloc[valid_idx])\n",
    "                preds_test += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "            elif train_fn == \"xgboost\":\n",
    "                dtrain = xgb.DMatrix(X.iloc[train_idx][xgb_features], label=y.iloc[train_idx])\n",
    "                dvalid = xgb.DMatrix(X.iloc[valid_idx][xgb_features], label=y.iloc[valid_idx])\n",
    "                model = xgb.train({**params, \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\"},\n",
    "                                  dtrain, num_boost_round=5000,\n",
    "                                  evals=[(dvalid, \"valid\")], early_stopping_rounds=100,\n",
    "                                  verbose_eval=False)\n",
    "                oof[valid_idx] = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n",
    "                preds_test += model.predict(xgb.DMatrix(X_test[xgb_features])) / N_SPLITS\n",
    "\n",
    "            elif train_fn == \"catboost\":\n",
    "                train_pool = Pool(X.iloc[train_idx], y.iloc[train_idx], cat_features=cat_cols)\n",
    "                valid_pool = Pool(X.iloc[valid_idx], y.iloc[valid_idx], cat_features=cat_cols)\n",
    "                model = CatBoostClassifier(**params, iterations=5000, eval_metric=\"AUC\", verbose=False)\n",
    "                model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100)\n",
    "                oof[valid_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "                preds_test += model.predict_proba(Pool(X_test, cat_features=cat_cols))[:, 1] / N_SPLITS\n",
    "\n",
    "        preds_test_all.append(preds_test)\n",
    "        oof_all.append(oof)\n",
    "    return np.array(oof_all), np.array(preds_test_all)\n",
    "\n",
    "oof_models, preds_models = train_oof_preds()\n",
    "\n",
    "# ================================\n",
    "# Blend weight tuning\n",
    "# ================================\n",
    "def tune_blend(trial):\n",
    "    w_lgb = trial.suggest_float(\"w_lgb\", 0, 1)\n",
    "    w_xgb = trial.suggest_float(\"w_xgb\", 0, 1)\n",
    "    w_cat = trial.suggest_float(\"w_cat\", 0, 1)\n",
    "    weights = np.array([w_lgb, w_xgb, w_cat])\n",
    "    weights /= weights.sum()\n",
    "    blended = np.average(oof_models, axis=0, weights=weights)\n",
    "    return roc_auc_score(y, blended)\n",
    "\n",
    "study_blend = optuna.create_study(direction=\"maximize\")\n",
    "study_blend.optimize(tune_blend, n_trials=50)\n",
    "\n",
    "# Final blended prediction\n",
    "best_w = np.array([study_blend.best_params[\"w_lgb\"],\n",
    "                   study_blend.best_params[\"w_xgb\"],\n",
    "                   study_blend.best_params[\"w_cat\"]])\n",
    "best_w /= best_w.sum()\n",
    "\n",
    "final_preds = np.average(preds_models, axis=0, weights=best_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# Submission\n",
    "# ================================\n",
    "sub = pd.DataFrame({id_col: test_f[id_col], TARGET_COL: final_preds})\n",
    "sub.to_csv(\"submission_step3_blend.csv\", index=False)\n",
    "print(\"Saved blended submission with weights:\", best_w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
