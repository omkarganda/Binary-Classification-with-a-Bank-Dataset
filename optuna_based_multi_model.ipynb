{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569df2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import warnings, io, sys, contextlib\n",
    "from tqdm.auto import tqdm\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, CatBoostError\n",
    "\n",
    "# ---------- global silence for Python warnings ----------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- context manager to silence stdout/stderr from libs ----------\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    saved_stdout, saved_stderr = sys.stdout, sys.stderr\n",
    "    try:\n",
    "        sys.stdout, sys.stderr = io.StringIO(), io.StringIO()\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = saved_stdout, saved_stderr\n",
    "\n",
    "# ---------- Optuna: silence its own logging; we’ll use its built-in progress bar ----------\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97e608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df = pd.read_csv(\"data/bank-full.csv\", sep = \";\")\n",
    "data_train = pd.read_csv(\"/teamspace/studios/this_studio/Binary-Classification-with-a-Bank-Dataset/data/train.csv\")\n",
    "data_train = data_train.drop([\"id\"], axis =1)\n",
    "# Convert 'y' column in original_df from 'yes'/'no' to 1/0\n",
    "# original_df['y'] = original_df['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Concatenate original_df and df (ignore index to avoid duplicate indices)\n",
    "# data_train = pd.concat([original_df, df], ignore_index=True)\n",
    "\n",
    "data_test = pd.read_csv(\"/teamspace/studios/this_studio/Binary-Classification-with-a-Bank-Dataset/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2385c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 10. Best value: 0.968223: 100%|██████████| 20/20 [18:16<00:00, 54.83s/it]\n",
      "Best trial: 15. Best value: 0.949196: 100%|██████████| 20/20 [26:37<00:00, 79.89s/it] \n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-08-19 01:44:55,347] Trial 0 failed with parameters: {'learning_rate': 0.03653747311121259, 'depth': 9, 'l2_leaf_reg': 8.22421335806522, 'subsample': 0.9114694136271042} because of the following error: CatBoostError(\"Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_7344/3861333143.py\", line 142, in tune_cat\n",
      "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100, verbose=False)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 5241, in fit\n",
      "    _process_synonyms(params)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 1645, in _process_synonyms\n",
      "    metric_period, verbose, logging_level = _process_verbose(\n",
      "                                            ~~~~~~~~~~~~~~~~^\n",
      "        metric_period, verbose, logging_level, verbose_eval, silent)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 236, in _process_verbose\n",
      "    raise CatBoostError('Only one of parameters {} should be set'.format(exclusive_params))\n",
      "_catboost.CatBoostError: Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_7344/3861333143.py\", line 147, in tune_cat\n",
      "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100, verbose=False)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 5241, in fit\n",
      "    _process_synonyms(params)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 1645, in _process_synonyms\n",
      "    metric_period, verbose, logging_level = _process_verbose(\n",
      "                                            ~~~~~~~~~~~~~~~~^\n",
      "        metric_period, verbose, logging_level, verbose_eval, silent)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py\", line 236, in _process_verbose\n",
      "    raise CatBoostError('Only one of parameters {} should be set'.format(exclusive_params))\n",
      "_catboost.CatBoostError: Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set\n",
      "[W 2025-08-19 01:44:55,352] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatBoostError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mtune_cat\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    141\u001b[39m     model = CatBoostClassifier(**params)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CatBoostError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:5241\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5240\u001b[39m params = \u001b[38;5;28mself\u001b[39m._init_params.copy()\n\u001b[32m-> \u001b[39m\u001b[32m5241\u001b[39m \u001b[43m_process_synonyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:1645\u001b[39m, in \u001b[36m_process_synonyms\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m   1643\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[33m'\u001b[39m\u001b[33msilent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m metric_period, verbose, logging_level = \u001b[43m_process_verbose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric_period \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:236\u001b[39m, in \u001b[36m_process_verbose\u001b[39m\u001b[34m(metric_period, verbose, logging_level, verbose_eval, silent)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m at_most_one > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m'\u001b[39m\u001b[33mOnly one of parameters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m should be set\u001b[39m\u001b[33m'\u001b[39m.format(exclusive_params))\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mCatBoostError\u001b[39m: Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCatBoostError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    158\u001b[39m study_xgb.optimize(tune_xgb, n_trials=\u001b[32m20\u001b[39m, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    160\u001b[39m study_cat = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[43mstudy_cat\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtune_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Train final models with best params and blend (single tqdm bar)\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_oof_preds\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mtune_cat\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    145\u001b[39m             params_cpu[\u001b[33m\"\u001b[39m\u001b[33mtask_type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mCPU\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m             model = CatBoostClassifier(**params_cpu)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m             \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         oof[va_idx] = model.predict_proba(valid_pool)[:, \u001b[32m1\u001b[39m]\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc_score(y, oof)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:5241\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5137\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5138\u001b[39m \u001b[33;03mFit the CatBoostClassifier model.\u001b[39;00m\n\u001b[32m   5139\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5237\u001b[39m \u001b[33;03mmodel : CatBoost\u001b[39;00m\n\u001b[32m   5238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5240\u001b[39m params = \u001b[38;5;28mself\u001b[39m._init_params.copy()\n\u001b[32m-> \u001b[39m\u001b[32m5241\u001b[39m \u001b[43m_process_synonyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:1645\u001b[39m, in \u001b[36m_process_synonyms\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m   1642\u001b[39m     silent = params[\u001b[33m'\u001b[39m\u001b[33msilent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m   1643\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[33m'\u001b[39m\u001b[33msilent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m metric_period, verbose, logging_level = \u001b[43m_process_verbose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric_period \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1649\u001b[39m     params[\u001b[33m'\u001b[39m\u001b[33mmetric_period\u001b[39m\u001b[33m'\u001b[39m] = metric_period\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/catboost/core.py:236\u001b[39m, in \u001b[36m_process_verbose\u001b[39m\u001b[34m(metric_period, verbose, logging_level, verbose_eval, silent)\u001b[39m\n\u001b[32m    234\u001b[39m at_most_one = \u001b[38;5;28msum\u001b[39m(params.get(exclusive) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m exclusive \u001b[38;5;129;01min\u001b[39;00m exclusive_params)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m at_most_one > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m'\u001b[39m\u001b[33mOnly one of parameters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m should be set\u001b[39m\u001b[33m'\u001b[39m.format(exclusive_params))\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mCatBoostError\u001b[39m: Only one of parameters ['verbose', 'logging_level', 'verbose_eval', 'silent'] should be set"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Data from Step 2\n",
    "# ================================\n",
    "train_f = data_train.copy()\n",
    "test_f = data_test.copy()\n",
    "\n",
    "TARGET_COL = \"y\"\n",
    "id_col = \"id\"\n",
    "train_f[TARGET_COL] = train_f[TARGET_COL].astype(int)\n",
    "\n",
    "# 2) Cast ALL object columns to 'category' and align categories across train/test\n",
    "all_obj_cols = [c for c in train_f.columns if train_f[c].dtype == \"O\"]\n",
    "for c in all_obj_cols:\n",
    "    train_f[c] = train_f[c].astype(\"category\")\n",
    "    if c in test_f.columns:\n",
    "        test_f[c] = test_f[c].astype(\"category\")\n",
    "\n",
    "# Also align categories for any pre-existing 'category' dtype columns (e.g., campaign_bins)\n",
    "all_cat_cols = [c for c in train_f.columns if str(train_f[c].dtype).startswith(\"category\")]\n",
    "for c in all_cat_cols:\n",
    "    if c in test_f.columns:\n",
    "        cats = sorted(list(set(train_f[c].cat.categories.tolist()) |\n",
    "                           set(test_f[c].cat.categories.tolist())))\n",
    "        train_f[c] = train_f[c].cat.set_categories(cats)\n",
    "        test_f[c]  = test_f[c].cat.set_categories(cats)\n",
    "\n",
    "# 3) Build feature matrices\n",
    "TARGET_COL = \"y\"\n",
    "features = [c for c in train_f.columns if c not in {TARGET_COL, id_col}]\n",
    "X = train_f[features].copy()\n",
    "y = train_f[TARGET_COL].copy()\n",
    "X_test = test_f[features].copy()\n",
    "\n",
    "# For LightGBM / CatBoost: list of categorical feature names + indices\n",
    "cat_cols = [c for c in X.columns if str(X[c].dtype).startswith(\"category\")]\n",
    "cat_idx  = [X.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# For XGBoost: ONLY numeric/boolean columns\n",
    "xgb_features = [c for c in X.columns if not str(X[c].dtype).startswith(\"category\")]\n",
    "# (LightGBM error stemmed from object dtypes; now fixed by casting to category)\n",
    "\n",
    "# ================================\n",
    "# CV setup\n",
    "# ================================\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ================================\n",
    "# Model tuning functions (silent; no prints; no logs)\n",
    "# ================================\n",
    "def tune_lgb(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0),\n",
    "        \"verbose\": -1,\n",
    "        \"is_unbalance\": True,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        dtr = lgb.Dataset(X.iloc[tr_idx], y.iloc[tr_idx], categorical_feature=cat_cols, free_raw_data=False)\n",
    "        dva = lgb.Dataset(X.iloc[va_idx], y.iloc[va_idx], categorical_feature=cat_cols, reference=dtr, free_raw_data=False)\n",
    "        with suppress_output():\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                dtr,\n",
    "                valid_sets=[dva],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(100, verbose=False),\n",
    "                    lgb.log_evaluation(0)  # no per-iter logs\n",
    "                ],\n",
    "            )\n",
    "            oof[va_idx] = model.predict(X.iloc[va_idx], num_iteration=model.best_iteration)\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "def tune_xgb(trial):\n",
    "    # Try GPU; if unavailable, silently fall back to CPU\n",
    "    base = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,  # silence XGBoost logs\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.0, 5.0),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\n",
    "        \"scale_pos_weight\": 1.0,\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        dtr = xgb.DMatrix(X.iloc[tr_idx][xgb_features], label=y.iloc[tr_idx])\n",
    "        dva = xgb.DMatrix(X.iloc[va_idx][xgb_features], label=y.iloc[va_idx])\n",
    "        params = dict(base)\n",
    "        params[\"device\"] = \"cuda\"\n",
    "        with suppress_output():\n",
    "            try:\n",
    "                model = xgb.train(params, dtr, num_boost_round=5000, evals=[(dva, \"valid\")],\n",
    "                                  early_stopping_rounds=100, verbose_eval=False)\n",
    "            except Exception:\n",
    "                params[\"device\"] = \"cpu\"\n",
    "                model = xgb.train(params, dtr, num_boost_round=5000, evals=[(dva, \"valid\")],\n",
    "                                  early_stopping_rounds=100, verbose_eval=False)\n",
    "            oof[va_idx] = model.predict(dva, iteration_range=(0, model.best_iteration))\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "def tune_cat(trial):\n",
    "    params = {\n",
    "        \"iterations\": 5000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"bootstrap_type\": \"Bernoulli\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"task_type\": \"GPU\",   # CatBoost will auto-fallback in our try/except\n",
    "        \"random_seed\": RANDOM_STATE,\n",
    "        \"verbose\": False,\n",
    "        \"logging_level\": \"Silent\",\n",
    "        \"allow_writing_files\": False,\n",
    "        \"use_best_model\": True,\n",
    "    }\n",
    "    oof = np.zeros(len(X))\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        train_pool = Pool(X.iloc[tr_idx], y.iloc[tr_idx], cat_features=cat_cols)\n",
    "        valid_pool = Pool(X.iloc[va_idx], y.iloc[va_idx], cat_features=cat_cols)\n",
    "        with suppress_output():\n",
    "            try:\n",
    "                model = CatBoostClassifier(**params)\n",
    "                model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100, verbose=False)\n",
    "            except CatBoostError:\n",
    "                params_cpu = dict(params)\n",
    "                params_cpu[\"task_type\"] = \"CPU\"\n",
    "                model = CatBoostClassifier(**params_cpu)\n",
    "                model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100, verbose=False)\n",
    "            oof[va_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "    return roc_auc_score(y, oof)\n",
    "\n",
    "# ================================\n",
    "# Run tuning with progress bars only\n",
    "# ================================\n",
    "study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "study_lgb.optimize(tune_lgb, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "study_xgb.optimize(tune_xgb, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "study_cat = optuna.create_study(direction=\"maximize\")\n",
    "study_cat.optimize(tune_cat, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "# ================================\n",
    "# Train final models with best params and blend (single tqdm bar)\n",
    "# ================================\n",
    "def train_oof_preds():\n",
    "    preds_test_all, oof_all = [], []\n",
    "    model_specs = [\n",
    "        (\"lgb\", study_lgb.best_params, \"lightgbm\"),\n",
    "        (\"xgb\", study_xgb.best_params, \"xgboost\"),\n",
    "        (\"cat\", study_cat.best_params, \"catboost\"),\n",
    "    ]\n",
    "    total_steps = len(model_specs) * N_SPLITS\n",
    "    with tqdm(total=total_steps, desc=\"Training blended models\", leave=True) as pbar:\n",
    "        for name, params, which in model_specs:\n",
    "            oof = np.zeros(len(X))\n",
    "            preds_test = np.zeros(len(X_test))\n",
    "            for tr_idx, va_idx in skf.split(X, y):\n",
    "                if which == \"lightgbm\":\n",
    "                    dtr = lgb.Dataset(X.iloc[tr_idx], y.iloc[tr_idx], categorical_feature=cat_cols, free_raw_data=False)\n",
    "                    dva = lgb.Dataset(X.iloc[va_idx], y.iloc[va_idx], categorical_feature=cat_cols, reference=dtr, free_raw_data=False)\n",
    "                    with suppress_output():\n",
    "                        model = lgb.train(\n",
    "                            {**params, \"objective\": \"binary\", \"metric\": \"auc\", \"verbose\": -1},\n",
    "                            dtr,\n",
    "                            valid_sets=[dva],\n",
    "                            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)],\n",
    "                        )\n",
    "                        oof[va_idx] = model.predict(X.iloc[va_idx], num_iteration=model.best_iteration)\n",
    "                        preds_test += model.predict(X_test, num_iteration=model.best_iteration) / N_SPLITS\n",
    "\n",
    "                elif which == \"xgboost\":\n",
    "                    dtr = xgb.DMatrix(X.iloc[tr_idx][xgb_features], label=y.iloc[tr_idx])\n",
    "                    dva = xgb.DMatrix(X.iloc[va_idx][xgb_features], label=y.iloc[va_idx])\n",
    "                    params_full = {**params, \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\", \"verbosity\": 0}\n",
    "                    if \"device\" not in params_full:\n",
    "                        params_full[\"device\"] = \"cuda\"\n",
    "                    with suppress_output():\n",
    "                        try:\n",
    "                            model = xgb.train(params_full, dtr, num_boost_round=5000,\n",
    "                                              evals=[(dva, \"valid\")], early_stopping_rounds=100, verbose_eval=False)\n",
    "                        except Exception:\n",
    "                            params_full[\"device\"] = \"cpu\"\n",
    "                            model = xgb.train(params_full, dtr, num_boost_round=5000,\n",
    "                                              evals=[(dva, \"valid\")], early_stopping_rounds=100, verbose_eval=False)\n",
    "                        oof[va_idx] = model.predict(dva, iteration_range=(0, model.best_iteration))\n",
    "                        preds_test += model.predict(xgb.DMatrix(X_test[xgb_features]), iteration_range=(0, model.best_iteration)) / N_SPLITS\n",
    "\n",
    "                elif which == \"catboost\":\n",
    "                    train_pool = Pool(X.iloc[tr_idx], y.iloc[tr_idx], cat_features=cat_cols)\n",
    "                    valid_pool = Pool(X.iloc[va_idx], y.iloc[va_idx], cat_features=cat_cols)\n",
    "                    with suppress_output():\n",
    "                        try:\n",
    "                            model = CatBoostClassifier(**params, iterations=5000, eval_metric=\"AUC\",\n",
    "                                                       verbose=False, logging_level=\"Silent\", allow_writing_files=False,\n",
    "                                                       use_best_model=True)\n",
    "                            model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100)\n",
    "                        except CatBoostError:\n",
    "                            model = CatBoostClassifier(**{**params, \"task_type\": \"CPU\"}, iterations=5000,\n",
    "                                                       eval_metric=\"AUC\", verbose=False, logging_level=\"Silent\",\n",
    "                                                       allow_writing_files=False, use_best_model=True)\n",
    "                            model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100)\n",
    "                        oof[va_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "                        preds_test += model.predict_proba(Pool(X_test, cat_features=cat_cols))[:, 1] / N_SPLITS\n",
    "                pbar.update(1)\n",
    "            preds_test_all.append(preds_test)\n",
    "            oof_all.append(oof)\n",
    "    return np.array(oof_all), np.array(preds_test_all)\n",
    "\n",
    "oof_models, preds_models = train_oof_preds()\n",
    "\n",
    "# ================================\n",
    "# Blend weight tuning (only Optuna progress bar)\n",
    "# ================================\n",
    "def tune_blend(trial):\n",
    "    w_lgb = trial.suggest_float(\"w_lgb\", 0, 1)\n",
    "    w_xgb = trial.suggest_float(\"w_xgb\", 0, 1)\n",
    "    w_cat = trial.suggest_float(\"w_cat\", 0, 1)\n",
    "    weights = np.array([w_lgb, w_xgb, w_cat], dtype=float)\n",
    "    if weights.sum() == 0:\n",
    "        weights = np.array([1.0, 1.0, 1.0])\n",
    "    weights /= weights.sum()\n",
    "    blended = np.average(oof_models, axis=0, weights=weights)\n",
    "    return roc_auc_score(y, blended)\n",
    "\n",
    "study_blend = optuna.create_study(direction=\"maximize\")\n",
    "study_blend.optimize(tune_blend, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "best_w = np.array([study_blend.best_params[\"w_lgb\"],\n",
    "                   study_blend.best_params[\"w_xgb\"],\n",
    "                   study_blend.best_params[\"w_cat\"]], dtype=float)\n",
    "if best_w.sum() == 0:\n",
    "    best_w = np.array([1.0, 1.0, 1.0])\n",
    "best_w /= best_w.sum()\n",
    "\n",
    "final_preds = np.average(preds_models, axis=0, weights=best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# Submission\n",
    "# ================================\n",
    "sub = pd.DataFrame({id_col: test_f[id_col], TARGET_COL: final_preds})\n",
    "sub.to_csv(\"submission_step3_blend.csv\", index=False)\n",
    "print(\"Saved blended submission with weights:\", best_w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
