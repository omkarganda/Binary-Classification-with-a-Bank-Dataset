{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a14bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28d88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"/teamspace/studios/this_studio/Binary-Classification-with-a-Bank-Dataset/data/bank-full.csv\", sep = \";\")\n",
    "df = pd.read_csv(\"/teamspace/studios/this_studio/Binary-Classification-with-a-Bank-Dataset/data/train.csv\")\n",
    "df = df.drop([\"id\"], axis =1)\n",
    "# Convert 'y' column in original_df from 'yes'/'no' to 1/0\n",
    "original_df['y'] = original_df['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Concatenate original_df and df (ignore index to avoid duplicate indices)\n",
    "data_train = pd.concat([original_df, df], ignore_index=True)\n",
    "\n",
    "data_test = pd.read_csv(\"/teamspace/studios/this_studio/Binary-Classification-with-a-Bank-Dataset/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4219b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2: Feature Engineering + Fold-safe Encodings (LightGBM) =====\n",
    "# Expects data_train and data_test in memory with target column 'y'\n",
    "# Outputs a new submission file with probabilities (you can threshold later if desired)\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "TARGET_COL = \"y\"\n",
    "ID_COL_CANDIDATES = [\"id\", \"ID\", \"Id\"]\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "WITH_DURATION = True            # Toggle to test with/without duration-related features\n",
    "TE_COLS = [\"job\", \"education\", \"contact\", \"month\", \"poutcome\", \"marital\"]  # target-encode\n",
    "FE_COLS = [\"job\", \"education\", \"contact\", \"month\", \"poutcome\", \"marital\"]  # frequency-encode\n",
    "TE_SMOOTH_M = 50.0             # smoothing strength for target encoding\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "def month_to_num(s):\n",
    "    m = str(s).strip().lower()\n",
    "    order = dict(jan=1,feb=2,mar=3,apr=4,may=5,jun=6,jul=7,aug=8,sep=9,oct=10,nov=11,dec=12)\n",
    "    return order.get(m, np.nan)\n",
    "\n",
    "def add_domain_features(df: pd.DataFrame, with_duration: bool = True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Contact history\n",
    "    pdays = out[\"pdays\"].copy()\n",
    "    contacted_before = (pdays != 999).astype(int)\n",
    "    out[\"contacted_before\"] = contacted_before\n",
    "\n",
    "    # Recency features (mask 999 as NaN)\n",
    "    pdays_masked = pdays.replace(999, np.nan)\n",
    "    out[\"days_since_last_contact\"] = pdays_masked.fillna(999)  # keep 999 as \"never\"\n",
    "    out[\"pdays_log1p\"] = np.log1p(pdays_masked).fillna(0)\n",
    "\n",
    "    # Contact intensity (safe divide)\n",
    "    out[\"contact_intensity\"] = (out[\"campaign\"] / (pdays_masked + 1)).fillna(0)\n",
    "\n",
    "    # Poutcome flags\n",
    "    if \"poutcome\" in out.columns:\n",
    "        pout = out[\"poutcome\"].astype(str).str.lower()\n",
    "        out[\"prev_success\"] = (pout == \"success\").astype(int)\n",
    "        out[\"prev_failure\"] = (pout == \"failure\").astype(int)\n",
    "        out[\"prev_unknown\"] = (pout == \"unknown\").astype(int)\n",
    "\n",
    "    # Month cyclical encoding\n",
    "    if \"month\" in out.columns:\n",
    "        mnum = out[\"month\"].map(month_to_num)\n",
    "        out[\"month_num\"] = mnum\n",
    "        out[\"month_sin\"] = np.sin(2 * np.pi * mnum / 12.0)\n",
    "        out[\"month_cos\"] = np.cos(2 * np.pi * mnum / 12.0)\n",
    "\n",
    "    # Day cyclical encoding (optional; keep both raw and cyc)\n",
    "    if \"day\" in out.columns:\n",
    "        day = out[\"day\"].clip(1, 31)\n",
    "        out[\"day_sin\"] = np.sin(2 * np.pi * day / 31.0)\n",
    "        out[\"day_cos\"] = np.cos(2 * np.pi * day / 31.0)\n",
    "\n",
    "    # Season flags (coarse)\n",
    "    if \"month_num\" in out.columns:\n",
    "        out[\"is_summer\"] = out[\"month_num\"].isin([6,7,8]).astype(int)\n",
    "        out[\"is_q4\"] = out[\"month_num\"].isin([10,11,12]).astype(int)\n",
    "\n",
    "    # Campaign bins\n",
    "    out[\"campaign_bins\"] = pd.cut(out[\"campaign\"],\n",
    "                                  bins=[-np.inf, 1, 3, 6, np.inf],\n",
    "                                  labels=[\"1\", \"2-3\", \"4-6\", \"7+\"],\n",
    "                                  ordered=True)\n",
    "\n",
    "    # Duration transforms\n",
    "    if with_duration and \"duration\" in out.columns:\n",
    "        out[\"duration_log1p\"] = np.log1p(out[\"duration\"].clip(lower=0))\n",
    "    else:\n",
    "        # if disabling, drop duration-derived features later\n",
    "        pass\n",
    "\n",
    "    # Balance transforms\n",
    "    # shift for log1p if negatives exist\n",
    "    shift = max(0, 1 - out[\"balance\"].min())\n",
    "    out[\"balance_log1p\"] = np.log1p(out[\"balance\"] + shift)\n",
    "    out[\"has_positive_balance\"] = (out[\"balance\"] > 0).astype(int)\n",
    "\n",
    "    # Interactions\n",
    "    if {\"housing\",\"loan\"}.issubset(out.columns):\n",
    "        out[\"housing_loan_combo\"] = (out[\"housing\"].astype(str).str.lower() + \"_\" +\n",
    "                                     out[\"loan\"].astype(str).str.lower())\n",
    "\n",
    "    if {\"job\",\"education\"}.issubset(out.columns):\n",
    "        out[\"job_x_education\"] = (out[\"job\"].astype(str).str.lower() + \"__\" +\n",
    "                                  out[\"education\"].astype(str).str.lower())\n",
    "\n",
    "    # Interaction with contacted_before\n",
    "    out[\"recency_x_campaign\"] = contacted_before * out[\"campaign\"]\n",
    "    if with_duration and \"duration_log1p\" in out.columns:\n",
    "        out[\"recent_and_long\"] = contacted_before * out[\"duration_log1p\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "def _make_te_map(X_tr_col: pd.Series, y_tr: pd.Series, m: float, prior: float):\n",
    "    # returns dict: category -> smoothed mean\n",
    "    stats = X_tr_col.to_frame(\"cat\").assign(y=y_tr.values).groupby(\"cat\")[\"y\"].agg([\"sum\",\"count\"])\n",
    "    te = (stats[\"sum\"] + prior * m) / (stats[\"count\"] + m)\n",
    "    return te.to_dict()\n",
    "\n",
    "def _apply_map(series: pd.Series, mapping: dict, default_val: float):\n",
    "    return series.map(mapping).fillna(default_val).astype(float)\n",
    "\n",
    "def add_fold_encodings(X_tr, y_tr, X_va, X_te,\n",
    "                       te_cols, fe_cols,\n",
    "                       te_smooth_m=50.0, strict_freq=True):\n",
    "    \"\"\"\n",
    "    Returns encoded copies with:\n",
    "      - target encoding: <col>_te\n",
    "      - frequency encoding: <col>_freq\n",
    "    TE and FE are fitted on X_tr only, then applied to X_va/X_te (no leakage).\n",
    "    \"\"\"\n",
    "    X_tr_e = X_tr.copy()\n",
    "    X_va_e = X_va.copy()\n",
    "    X_te_e = X_te.copy()\n",
    "\n",
    "    # Target Encoding\n",
    "    prior = y_tr.mean()\n",
    "    for col in te_cols:\n",
    "        if col not in X_tr_e.columns:\n",
    "            continue\n",
    "        te_map = _make_te_map(X_tr_e[col].astype(str), y_tr, te_smooth_m, prior)\n",
    "        default_val = prior\n",
    "        X_tr_e[f\"{col}_te\"] = _apply_map(X_tr_e[col].astype(str), te_map, default_val)\n",
    "        X_va_e[f\"{col}_te\"] = _apply_map(X_va_e[col].astype(str), te_map, default_val)\n",
    "        X_te_e[f\"{col}_te\"] = _apply_map(X_te_e[col].astype(str), te_map, default_val)\n",
    "\n",
    "    # Frequency Encoding\n",
    "    for col in fe_cols:\n",
    "        if col not in X_tr_e.columns:\n",
    "            continue\n",
    "        tr_counts = X_tr_e[col].astype(str).value_counts(dropna=False)\n",
    "        tr_freq = (tr_counts / tr_counts.sum()).to_dict()\n",
    "        default_freq = 0.0 if strict_freq else (1.0 / max(1, len(tr_counts)))\n",
    "        X_tr_e[f\"{col}_freq\"] = X_tr_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
    "        X_va_e[f\"{col}_freq\"] = X_va_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
    "        X_te_e[f\"{col}_freq\"] = X_te_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
    "\n",
    "    return X_tr_e, X_va_e, X_te_e\n",
    "\n",
    "# --------------------\n",
    "# Data prep\n",
    "# --------------------\n",
    "train = data_train.copy()\n",
    "test  = data_test.copy()\n",
    "\n",
    "# Map y from \"yes\"/\"no\" to 1/0 if needed\n",
    "if train[TARGET_COL].dtype == \"O\":\n",
    "    train[TARGET_COL] = train[TARGET_COL].str.strip().str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Identify ID column (fallback to index)\n",
    "id_col = next((c for c in ID_COL_CANDIDATES if c in test.columns), None)\n",
    "if id_col is None:\n",
    "    id_col = \"id\"\n",
    "    test[id_col] = np.arange(len(test))\n",
    "\n",
    "# Base engineered features (no target leakage)\n",
    "train_f = add_domain_features(train, WITH_DURATION)\n",
    "test_f  = add_domain_features(test,  WITH_DURATION)\n",
    "\n",
    "# Optionally drop duration and its derivatives when WITH_DURATION=False\n",
    "if not WITH_DURATION:\n",
    "    drop_no_duration = [c for c in train_f.columns if c.startswith(\"duration\")]\n",
    "    drop_no_duration += [\"recent_and_long\"]\n",
    "    train_f = train_f.drop(columns=[c for c in drop_no_duration if c in train_f.columns])\n",
    "    test_f  = test_f.drop(columns=[c for c in drop_no_duration if c in test_f.columns])\n",
    "\n",
    "# Cast string categoricals to category dtype so LightGBM can natively handle them\n",
    "cat_cols = []\n",
    "for c in train_f.columns:\n",
    "    if c == TARGET_COL or c == id_col:\n",
    "        continue\n",
    "    if train_f[c].dtype == \"O\" or str(train_f[c].dtype).startswith(\"category\"):\n",
    "        # mark as categorical if itâ€™s textual or already category (avoid explicitly numeric engineered cols)\n",
    "        cat_cols.append(c)\n",
    "\n",
    "# Ensure same categories across train/test for categorical columns\n",
    "for c in cat_cols:\n",
    "    train_f[c] = train_f[c].astype(\"category\")\n",
    "    test_f[c]  = test_f[c].astype(\"category\")\n",
    "    all_cats = sorted(list(set(train_f[c].cat.categories.tolist()) | set(test_f[c].cat.categories.tolist())))\n",
    "    train_f[c] = train_f[c].cat.set_categories(all_cats)\n",
    "    test_f[c]  = test_f[c].cat.set_categories(all_cats)\n",
    "\n",
    "# Define features (excluding target & id)\n",
    "features = [c for c in train_f.columns if c not in {TARGET_COL, id_col}]\n",
    "\n",
    "X_full = train_f[features]\n",
    "y_full = train_f[TARGET_COL].astype(int)\n",
    "X_test_full = test_f[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3210db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.965274\tvalid's auc: 0.963336\n",
      "[400]\ttrain's auc: 0.969284\tvalid's auc: 0.965564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m dtr = lgb.Dataset(X_tr_e[fold_features], label=y_tr, categorical_feature=fold_cat_cols, free_raw_data=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     47\u001b[39m dva = lgb.Dataset(X_va_e[fold_features], label=y_va, categorical_feature=fold_cat_cols, reference=dtr, free_raw_data=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdva\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m oof[va_idx] = model.predict(X_va_e[fold_features], num_iteration=model.best_iteration)\n\u001b[32m     62\u001b[39m fold_auc = roc_auc_score(y_va, oof[va_idx])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/lightgbm/engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.13/site-packages/lightgbm/basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------\n",
    "# CV + Training with fold-safe encodings\n",
    "# --------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 64,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"max_depth\": -1,\n",
    "    \"verbose\": -1,\n",
    "    \"is_unbalance\": True,\n",
    "    \"seed\": RANDOM_STATE\n",
    "}\n",
    "\n",
    "oof = np.zeros(len(X_full))\n",
    "test_pred = np.zeros(len(X_test_full))\n",
    "fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_full, y_full), 1):\n",
    "    X_tr, y_tr = X_full.iloc[tr_idx].copy(), y_full.iloc[tr_idx].copy()\n",
    "    X_va, y_va = X_full.iloc[va_idx].copy(), y_full.iloc[va_idx].copy()\n",
    "    X_te = X_test_full.copy()\n",
    "\n",
    "    # Add fold-safe target & frequency encodings\n",
    "    X_tr_e, X_va_e, X_te_e = add_fold_encodings(\n",
    "        X_tr, y_tr, X_va, X_te,\n",
    "        te_cols=TE_COLS + [\"housing_loan_combo\", \"job_x_education\", \"campaign_bins\"],\n",
    "        fe_cols=FE_COLS + [\"housing_loan_combo\", \"job_x_education\", \"campaign_bins\"],\n",
    "        te_smooth_m=TE_SMOOTH_M,\n",
    "        strict_freq=True\n",
    "    )\n",
    "\n",
    "    # Update feature list for this fold (original + new encodings)\n",
    "    fold_features = list(X_tr_e.columns)\n",
    "\n",
    "    # Categorical columns for LGBM (only those that are truly categories in this fold)\n",
    "    fold_cat_cols = [c for c in fold_features if (c in cat_cols and c in X_tr_e.columns)]\n",
    "\n",
    "    dtr = lgb.Dataset(X_tr_e[fold_features], label=y_tr, categorical_feature=fold_cat_cols, free_raw_data=False)\n",
    "    dva = lgb.Dataset(X_va_e[fold_features], label=y_va, categorical_feature=fold_cat_cols, reference=dtr, free_raw_data=False)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[dtr, dva],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(200),\n",
    "            lgb.log_evaluation(200)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof[va_idx] = model.predict(X_va_e[fold_features], num_iteration=model.best_iteration)\n",
    "    fold_auc = roc_auc_score(y_va, oof[va_idx])\n",
    "    fold_aucs.append(fold_auc)\n",
    "    print(f\"[Fold {fold}] AUC: {fold_auc:.5f}\")\n",
    "\n",
    "    test_pred += model.predict(X_te_e[fold_features], num_iteration=model.best_iteration) / N_SPLITS\n",
    "\n",
    "cv_auc = roc_auc_score(y_full, oof)\n",
    "print(f\"\\nOOF AUC: {cv_auc:.5f} | Folds: {', '.join(f'{a:.5f}' for a in fold_aucs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b90a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Submission\n",
    "# --------------------\n",
    "sub = pd.DataFrame({\n",
    "    id_col: test[id_col].values,\n",
    "    TARGET_COL: test_pred  # probabilities; threshold later to get 0/1 if needed\n",
    "})\n",
    "sub_path = \"submission_step2_feat_eng.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"Saved: {sub_path}\")\n",
    "\n",
    "# If you want to save a 0/1 version immediately as well, uncomment:\n",
    "# sub_bin = sub.copy()\n",
    "# sub_bin[TARGET_COL] = (sub_bin[TARGET_COL] >= 0.5).astype(int)\n",
    "# sub_bin_path = \"submission_step2_feat_eng_binary.csv\"\n",
    "# sub_bin.to_csv(sub_bin_path, index=False)\n",
    "# print(f\"Saved: {sub_bin_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d365d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.964724\tvalid's auc: 0.963383\n",
      "[400]\ttrain's auc: 0.968476\tvalid's auc: 0.965588\n",
      "[600]\ttrain's auc: 0.970503\tvalid's auc: 0.966252\n",
      "[800]\ttrain's auc: 0.972134\tvalid's auc: 0.966607\n",
      "[1000]\ttrain's auc: 0.973609\tvalid's auc: 0.966909\n",
      "[1200]\ttrain's auc: 0.974898\tvalid's auc: 0.967112\n",
      "[1400]\ttrain's auc: 0.976102\tvalid's auc: 0.967247\n",
      "[1600]\ttrain's auc: 0.977181\tvalid's auc: 0.967346\n",
      "[1800]\ttrain's auc: 0.978197\tvalid's auc: 0.967437\n",
      "[2000]\ttrain's auc: 0.979152\tvalid's auc: 0.967473\n",
      "[2200]\ttrain's auc: 0.980019\tvalid's auc: 0.967505\n",
      "[2400]\ttrain's auc: 0.980885\tvalid's auc: 0.967555\n",
      "[2600]\ttrain's auc: 0.981672\tvalid's auc: 0.967544\n",
      "Early stopping, best iteration is:\n",
      "[2427]\ttrain's auc: 0.981001\tvalid's auc: 0.967561\n",
      "[Fold 1] AUC: 0.96756\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.964613\tvalid's auc: 0.96362\n",
      "[400]\ttrain's auc: 0.968346\tvalid's auc: 0.965789\n",
      "[600]\ttrain's auc: 0.970353\tvalid's auc: 0.966433\n",
      "[800]\ttrain's auc: 0.972004\tvalid's auc: 0.966811\n",
      "[1000]\ttrain's auc: 0.973404\tvalid's auc: 0.967032\n",
      "[1200]\ttrain's auc: 0.974718\tvalid's auc: 0.967234\n",
      "[1400]\ttrain's auc: 0.975914\tvalid's auc: 0.967337\n",
      "[1600]\ttrain's auc: 0.976985\tvalid's auc: 0.967409\n",
      "[1800]\ttrain's auc: 0.977988\tvalid's auc: 0.967473\n",
      "[2000]\ttrain's auc: 0.978948\tvalid's auc: 0.967539\n",
      "[2200]\ttrain's auc: 0.979856\tvalid's auc: 0.967583\n",
      "[2400]\ttrain's auc: 0.980703\tvalid's auc: 0.967598\n",
      "[2600]\ttrain's auc: 0.981476\tvalid's auc: 0.967575\n",
      "Early stopping, best iteration is:\n",
      "[2429]\ttrain's auc: 0.980826\tvalid's auc: 0.967606\n",
      "[Fold 2] AUC: 0.96761\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.964822\tvalid's auc: 0.962918\n",
      "[400]\ttrain's auc: 0.96872\tvalid's auc: 0.965273\n",
      "[600]\ttrain's auc: 0.970748\tvalid's auc: 0.965893\n",
      "[800]\ttrain's auc: 0.972394\tvalid's auc: 0.966329\n",
      "[1000]\ttrain's auc: 0.973774\tvalid's auc: 0.966568\n",
      "[1200]\ttrain's auc: 0.97507\tvalid's auc: 0.966762\n",
      "[1400]\ttrain's auc: 0.976202\tvalid's auc: 0.966891\n",
      "[1600]\ttrain's auc: 0.977303\tvalid's auc: 0.967002\n",
      "[1800]\ttrain's auc: 0.978304\tvalid's auc: 0.967038\n",
      "[2000]\ttrain's auc: 0.979255\tvalid's auc: 0.967083\n",
      "[2200]\ttrain's auc: 0.980146\tvalid's auc: 0.967117\n",
      "[2400]\ttrain's auc: 0.980995\tvalid's auc: 0.967111\n",
      "[2600]\ttrain's auc: 0.981787\tvalid's auc: 0.967148\n",
      "[2800]\ttrain's auc: 0.982559\tvalid's auc: 0.967171\n",
      "[3000]\ttrain's auc: 0.983279\tvalid's auc: 0.967187\n",
      "[3200]\ttrain's auc: 0.983958\tvalid's auc: 0.9672\n",
      "[3400]\ttrain's auc: 0.98461\tvalid's auc: 0.967215\n",
      "[3600]\ttrain's auc: 0.985244\tvalid's auc: 0.967208\n",
      "Early stopping, best iteration is:\n",
      "[3509]\ttrain's auc: 0.984964\tvalid's auc: 0.967226\n",
      "[Fold 3] AUC: 0.96723\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.964855\tvalid's auc: 0.963248\n",
      "[400]\ttrain's auc: 0.968493\tvalid's auc: 0.965456\n",
      "[600]\ttrain's auc: 0.970552\tvalid's auc: 0.966164\n",
      "[800]\ttrain's auc: 0.972218\tvalid's auc: 0.966587\n",
      "[1000]\ttrain's auc: 0.973676\tvalid's auc: 0.966879\n",
      "[1200]\ttrain's auc: 0.974967\tvalid's auc: 0.967085\n",
      "[1400]\ttrain's auc: 0.976148\tvalid's auc: 0.967262\n",
      "[1600]\ttrain's auc: 0.977213\tvalid's auc: 0.967357\n",
      "[1800]\ttrain's auc: 0.978237\tvalid's auc: 0.967429\n",
      "[2000]\ttrain's auc: 0.979173\tvalid's auc: 0.967477\n",
      "[2200]\ttrain's auc: 0.980057\tvalid's auc: 0.967515\n",
      "[2400]\ttrain's auc: 0.980943\tvalid's auc: 0.967553\n",
      "[2600]\ttrain's auc: 0.981745\tvalid's auc: 0.967589\n",
      "[2800]\ttrain's auc: 0.982494\tvalid's auc: 0.96759\n",
      "[3000]\ttrain's auc: 0.983219\tvalid's auc: 0.96761\n",
      "[3200]\ttrain's auc: 0.983909\tvalid's auc: 0.967619\n",
      "[3400]\ttrain's auc: 0.984547\tvalid's auc: 0.967604\n",
      "Early stopping, best iteration is:\n",
      "[3225]\ttrain's auc: 0.983993\tvalid's auc: 0.967629\n",
      "[Fold 4] AUC: 0.96763\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's auc: 0.964783\tvalid's auc: 0.962947\n",
      "[400]\ttrain's auc: 0.968626\tvalid's auc: 0.965305\n",
      "[600]\ttrain's auc: 0.970647\tvalid's auc: 0.965962\n",
      "[800]\ttrain's auc: 0.972281\tvalid's auc: 0.966344\n",
      "[1000]\ttrain's auc: 0.973738\tvalid's auc: 0.966583\n",
      "[1200]\ttrain's auc: 0.975017\tvalid's auc: 0.966743\n",
      "[1400]\ttrain's auc: 0.976193\tvalid's auc: 0.966862\n",
      "[1600]\ttrain's auc: 0.977279\tvalid's auc: 0.96696\n",
      "[1800]\ttrain's auc: 0.978277\tvalid's auc: 0.967059\n",
      "[2000]\ttrain's auc: 0.979226\tvalid's auc: 0.967116\n",
      "[2200]\ttrain's auc: 0.980121\tvalid's auc: 0.967139\n",
      "[2400]\ttrain's auc: 0.980976\tvalid's auc: 0.967164\n",
      "[2600]\ttrain's auc: 0.981767\tvalid's auc: 0.96716\n",
      "[2800]\ttrain's auc: 0.982536\tvalid's auc: 0.967161\n",
      "Early stopping, best iteration is:\n",
      "[2649]\ttrain's auc: 0.981959\tvalid's auc: 0.967177\n",
      "[Fold 5] AUC: 0.96718\n",
      "\n",
      "OOF AUC: 0.96742 | Folds: 0.96756, 0.96761, 0.96723, 0.96763, 0.96718\n",
      "Saved: submission_lgbm_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Config ---\n",
    "TARGET_COL = \"y\"\n",
    "ID_COL_CANDIDATES = [\"id\", \"ID\", \"Id\"]\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Safety checks & light cleaning ---\n",
    "train = data_train.copy()\n",
    "test  = data_test.copy()\n",
    "\n",
    "# Map y from \"yes\"/\"no\" to 1/0 if needed\n",
    "if train[TARGET_COL].dtype == \"O\":\n",
    "    train[TARGET_COL] = train[TARGET_COL].str.strip().str.lower().map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Identify ID column (fallback to index)\n",
    "id_col = None\n",
    "for c in ID_COL_CANDIDATES:\n",
    "    if c in test.columns:\n",
    "        id_col = c\n",
    "        break\n",
    "if id_col is None:\n",
    "    id_col = \"id\"\n",
    "    test[id_col] = np.arange(len(test))\n",
    "\n",
    "# Features: drop target + id if present\n",
    "drop_cols = {TARGET_COL, id_col} & set(train.columns)\n",
    "features = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "# Find categoricals and cast to category (LightGBM-native handling)\n",
    "cat_cols = [c for c in features if train[c].dtype == \"O\"]\n",
    "for c in cat_cols:\n",
    "    train[c] = train[c].astype(\"category\")\n",
    "    if c in test.columns:\n",
    "        test[c] = test[c].astype(\"category\")\n",
    "\n",
    "# Align categorical levels across train/test\n",
    "for c in cat_cols:\n",
    "    all_cats = sorted(list(set(train[c].cat.categories.tolist()) | set(test[c].cat.categories.tolist())))\n",
    "    train[c] = train[c].cat.set_categories(all_cats)\n",
    "    test[c]  = test[c].cat.set_categories(all_cats)\n",
    "\n",
    "X = train[features]\n",
    "y = train[TARGET_COL].astype(int)\n",
    "X_test = test[features]\n",
    "\n",
    "# --- CV & Training ---\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 64,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"max_depth\": -1,\n",
    "    \"verbose\": -1,\n",
    "    \"is_unbalance\": True,\n",
    "    \"seed\": RANDOM_STATE\n",
    "}\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "test_pred = np.zeros(len(X_test))\n",
    "fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n",
    "\n",
    "    dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    dva = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, reference=dtr, free_raw_data=False)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[dtr, dva],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(200),\n",
    "            lgb.log_evaluation(200)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    oof[va_idx] = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    fold_auc = roc_auc_score(y_va, oof[va_idx])\n",
    "    fold_aucs.append(fold_auc)\n",
    "    print(f\"[Fold {fold}] AUC: {fold_auc:.5f}\")\n",
    "\n",
    "    test_pred += model.predict(X_test, num_iteration=model.best_iteration) / N_SPLITS\n",
    "\n",
    "cv_auc = roc_auc_score(y, oof)\n",
    "print(f\"\\nOOF AUC: {cv_auc:.5f} | Folds: {', '.join(f'{a:.5f}' for a in fold_aucs)}\")\n",
    "\n",
    "# --- Submission file ---\n",
    "sub = pd.DataFrame({\n",
    "    id_col: test[id_col].values,\n",
    "    TARGET_COL: test_pred\n",
    "})\n",
    "sub_path = \"submission_lgbm_baseline.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"Saved: {sub_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated predictions in submission_lgbm_baseline.csv to binary 0/1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your existing predictions file\n",
    "sub_path = \"submission_lgbm_baseline.csv\"\n",
    "\n",
    "# Read the file\n",
    "sub = pd.read_csv(sub_path)\n",
    "\n",
    "# Convert predictions to 0/1\n",
    "sub[\"y\"] = (sub[\"y\"] >= 0.50).astype(int)\n",
    "\n",
    "# Save updated predictions\n",
    "sub.to_csv(\"submission_lgbm_baseline_binary_50.csv\", index=False)\n",
    "print(f\"Updated predictions in {sub_path} to binary 0/1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
