{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary Classification with Bank Marketing Dataset\n",
        "\n",
        "## Professional Machine Learning Pipeline with Advanced Feature Engineering and Hyperparameter Optimization\n",
        "\n",
        "### Overview\n",
        "This notebook presents a comprehensive machine learning solution for binary classification using a bank marketing dataset. The goal is to predict whether a client will subscribe to a term deposit based on various demographic, financial, and campaign-related features.\n",
        "\n",
        "### Key Features:\n",
        "- **Comprehensive Exploratory Data Analysis (EDA)**\n",
        "- **Advanced Feature Engineering** with domain-specific transformations\n",
        "- **Multi-Model Approach** using LightGBM, XGBoost, and CatBoost\n",
        "- **Automated Hyperparameter Optimization** with Optuna\n",
        "- **Model Ensembling** with optimized blending weights\n",
        "- **Cross-Validation** for robust performance estimation\n",
        "\n",
        "### Methodology:\n",
        "1. Data Loading and Initial Analysis\n",
        "2. Exploratory Data Analysis\n",
        "3. Feature Engineering and Preprocessing\n",
        "4. Model Development with Hyperparameter Tuning\n",
        "5. Model Ensembling and Final Predictions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import io\n",
        "import sys\n",
        "import contextlib\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier, Pool, CatBoostError\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Configure display and warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"📊 Using random state: {RANDOM_STATE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Configuration\n",
        "\n",
        "Let's load the dataset and set up our configuration parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "CONFIG = {\n",
        "    'target_col': 'y',\n",
        "    'id_col': 'id',\n",
        "    'n_splits': 5,\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'optuna_trials': 50,  # Number of trials for hyperparameter optimization\n",
        "    'early_stopping_rounds': 100,\n",
        "    'with_duration': True,  # Toggle duration-based features\n",
        "    'te_smooth_m': 50.0,    # Target encoding smoothing parameter\n",
        "    'verbose': True\n",
        "}\n",
        "\n",
        "# Context manager to suppress model training outputs when needed\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    \"\"\"Context manager to suppress stdout/stderr from libraries\"\"\"\n",
        "    saved_stdout, saved_stderr = sys.stdout, sys.stderr\n",
        "    try:\n",
        "        sys.stdout, sys.stderr = io.StringIO(), io.StringIO()\n",
        "        yield\n",
        "    finally:\n",
        "        sys.stdout, sys.stderr = saved_stdout, saved_stderr\n",
        "\n",
        "# Silence Optuna's own logging for cleaner output\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "print(\"⚙️ Configuration loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "print(\"📁 Loading datasets...\")\n",
        "\n",
        "try:\n",
        "    # Load training data\n",
        "    data_train = pd.read_csv(\"data/train.csv\")\n",
        "    print(f\"   Training data loaded: {data_train.shape}\")\n",
        "    \n",
        "    # Load test data  \n",
        "    data_test = pd.read_csv(\"data/test.csv\")\n",
        "    print(f\"   Test data loaded: {data_test.shape}\")\n",
        "    \n",
        "    # Load additional bank data if available\n",
        "    try:\n",
        "        bank_full = pd.read_csv(\"data/bank-full.csv\", sep=\";\")\n",
        "        print(f\"   Bank-full data loaded: {bank_full.shape}\")\n",
        "        \n",
        "        # Convert target variable and combine datasets\n",
        "        bank_full[CONFIG['target_col']] = bank_full[CONFIG['target_col']].map({'yes': 1, 'no': 0})\n",
        "        \n",
        "        # Remove ID column from training data if present\n",
        "        if CONFIG['id_col'] in data_train.columns:\n",
        "            data_train = data_train.drop(CONFIG['id_col'], axis=1)\n",
        "            \n",
        "        # Combine datasets for more training data\n",
        "        data_train = pd.concat([bank_full, data_train], ignore_index=True)\n",
        "        print(f\"   Combined training data: {data_train.shape}\")\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"   bank-full.csv not found, proceeding with train.csv only\")\n",
        "        if CONFIG['id_col'] in data_train.columns:\n",
        "            data_train = data_train.drop(CONFIG['id_col'], axis=1)\n",
        "    \n",
        "    print(\"✅ Data loading completed!\")\n",
        "    \n",
        "    # Basic dataset info\n",
        "    print(f\"\\n📊 Dataset Overview:\")\n",
        "    print(f\"   Training samples: {len(data_train):,}\")\n",
        "    print(f\"   Test samples: {len(data_test):,}\")\n",
        "    print(f\"   Features: {data_train.shape[1] - 1}\")  # -1 for target column\n",
        "    \n",
        "    # Check target distribution\n",
        "    if CONFIG['target_col'] in data_train.columns:\n",
        "        target_dist = data_train[CONFIG['target_col']].value_counts(normalize=True)\n",
        "        print(f\"   Target distribution: {target_dist.to_dict()}\")\n",
        "        \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's perform a comprehensive analysis of our dataset to understand the distribution of features, identify outliers, and discover patterns that will inform our feature engineering strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic data exploration\n",
        "print(\"🔍 Basic Data Exploration\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display basic info\n",
        "print(f\"Dataset shape: {data_train.shape}\")\n",
        "print(f\"Memory usage: {data_train.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Data types and missing values\n",
        "print(\"\\n📋 Data Types and Missing Values:\")\n",
        "print(\"-\" * 40)\n",
        "info_df = pd.DataFrame({\n",
        "    'Data Type': data_train.dtypes,\n",
        "    'Missing Values': data_train.isnull().sum(),\n",
        "    'Missing %': (data_train.isnull().sum() / len(data_train)) * 100,\n",
        "    'Unique Values': data_train.nunique()\n",
        "})\n",
        "print(info_df)\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = data_train.duplicated().sum()\n",
        "print(f\"\\n🔄 Duplicate rows: {duplicates} ({duplicates/len(data_train)*100:.2f}%)\")\n",
        "\n",
        "# Target variable analysis\n",
        "print(f\"\\n🎯 Target Variable ('{CONFIG['target_col']}') Analysis:\")\n",
        "print(\"-\" * 40)\n",
        "target_counts = data_train[CONFIG['target_col']].value_counts()\n",
        "target_pct = data_train[CONFIG['target_col']].value_counts(normalize=True) * 100\n",
        "\n",
        "for val, count in target_counts.items():\n",
        "    print(f\"   Class {val}: {count:,} samples ({target_pct[val]:.1f}%)\")\n",
        "    \n",
        "class_ratio = target_counts[0] / target_counts[1] if len(target_counts) > 1 else 1\n",
        "print(f\"   Class imbalance ratio: {class_ratio:.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric and categorical features\n",
        "numeric_features = data_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if CONFIG['target_col'] in numeric_features:\n",
        "    numeric_features.remove(CONFIG['target_col'])\n",
        "\n",
        "categorical_features = data_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f\"📊 Feature Types:\")\n",
        "print(f\"   Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"   Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "# Detailed analysis of numeric features\n",
        "print(\"\\n📈 Numeric Features Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "numeric_stats = data_train[numeric_features].describe()\n",
        "print(numeric_stats)\n",
        "\n",
        "# Outlier detection for numeric features\n",
        "print(\"\\n🎯 Outlier Detection (IQR Method):\")\n",
        "print(\"-\" * 40)\n",
        "outlier_summary = {}\n",
        "\n",
        "for col in numeric_features:\n",
        "    Q1 = data_train[col].quantile(0.25)\n",
        "    Q3 = data_train[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = ((data_train[col] < lower_bound) | (data_train[col] > upper_bound)).sum()\n",
        "    outlier_pct = (outliers / len(data_train)) * 100\n",
        "    \n",
        "    outlier_summary[col] = {\n",
        "        'count': outliers,\n",
        "        'percentage': outlier_pct,\n",
        "        'bounds': (lower_bound, upper_bound)\n",
        "    }\n",
        "    \n",
        "    print(f\"   {col:>12}: {outliers:,} outliers ({outlier_pct:.1f}%)\")\n",
        "\n",
        "# Skewness analysis\n",
        "print(\"\\n📐 Skewness Analysis:\")\n",
        "print(\"-\" * 40)\n",
        "for col in numeric_features:\n",
        "    skewness = stats.skew(data_train[col])\n",
        "    print(f\"   {col:>12}: {skewness:.3f} {'(highly skewed)' if abs(skewness) > 1 else '(moderately skewed)' if abs(skewness) > 0.5 else '(normal)'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "print(\"🏷️ Categorical Features Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "cat_analysis = {}\n",
        "for col in categorical_features:\n",
        "    unique_vals = data_train[col].nunique()\n",
        "    mode_val = data_train[col].mode().iloc[0] if len(data_train[col].mode()) > 0 else 'N/A'\n",
        "    mode_pct = (data_train[col] == mode_val).mean() * 100\n",
        "    \n",
        "    cat_analysis[col] = {\n",
        "        'unique_values': unique_vals,\n",
        "        'mode': mode_val,\n",
        "        'mode_percentage': mode_pct\n",
        "    }\n",
        "    \n",
        "    print(f\"   {col:>12}: {unique_vals} unique values, mode='{mode_val}' ({mode_pct:.1f}%)\")\n",
        "    \n",
        "    # Show value counts for features with reasonable number of categories\n",
        "    if unique_vals <= 10:\n",
        "        print(f\"                 Distribution:\")\n",
        "        value_counts = data_train[col].value_counts()\n",
        "        for val, count in value_counts.head().items():\n",
        "            pct = (count / len(data_train)) * 100\n",
        "            print(f\"                   '{val}': {count:,} ({pct:.1f}%)\")\n",
        "        if len(value_counts) > 5:\n",
        "            print(f\"                   ... and {len(value_counts) - 5} more categories\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations for EDA\n",
        "def create_eda_plots():\n",
        "    \"\"\"Create comprehensive EDA visualizations\"\"\"\n",
        "    \n",
        "    # Set up the plotting style\n",
        "    plt.rcParams['figure.figsize'] = (15, 10)\n",
        "    \n",
        "    # 1. Target distribution\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Target Variable Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Target counts\n",
        "    target_counts = data_train[CONFIG['target_col']].value_counts()\n",
        "    axes[0,0].bar(target_counts.index, target_counts.values, color=['lightcoral', 'lightblue'])\n",
        "    axes[0,0].set_title('Target Distribution (Counts)')\n",
        "    axes[0,0].set_xlabel('Target Class')\n",
        "    axes[0,0].set_ylabel('Count')\n",
        "    for i, v in enumerate(target_counts.values):\n",
        "        axes[0,0].text(i, v + max(target_counts.values)*0.01, str(v), ha='center', va='bottom')\n",
        "    \n",
        "    # Target percentages (pie chart)\n",
        "    axes[0,1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
        "                  colors=['lightcoral', 'lightblue'], startangle=90)\n",
        "    axes[0,1].set_title('Target Distribution (Percentages)')\n",
        "    \n",
        "    # Age distribution by target\n",
        "    if 'age' in data_train.columns:\n",
        "        for target_val in data_train[CONFIG['target_col']].unique():\n",
        "            data_subset = data_train[data_train[CONFIG['target_col']] == target_val]\n",
        "            axes[1,0].hist(data_subset['age'], alpha=0.6, label=f'Target={target_val}', bins=30)\n",
        "        axes[1,0].set_title('Age Distribution by Target')\n",
        "        axes[1,0].set_xlabel('Age')\n",
        "        axes[1,0].set_ylabel('Frequency')\n",
        "        axes[1,0].legend()\n",
        "    \n",
        "    # Duration vs target (if available)\n",
        "    if 'duration' in data_train.columns:\n",
        "        data_train.boxplot(column='duration', by=CONFIG['target_col'], ax=axes[1,1])\n",
        "        axes[1,1].set_title('Duration Distribution by Target')\n",
        "        axes[1,1].set_xlabel('Target Class')\n",
        "        axes[1,1].set_ylabel('Duration')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Correlation matrix for numeric features\n",
        "    if len(numeric_features) > 1:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        correlation_matrix = data_train[numeric_features + [CONFIG['target_col']]].corr()\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "                   square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # 3. Distribution plots for key numeric features\n",
        "    key_numeric = numeric_features[:6]  # Show first 6 numeric features\n",
        "    if key_numeric:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        axes = axes.ravel()\n",
        "        \n",
        "        for i, col in enumerate(key_numeric):\n",
        "            if i < len(axes):\n",
        "                # Histogram with KDE\n",
        "                axes[i].hist(data_train[col], bins=30, alpha=0.7, color='skyblue', density=True)\n",
        "                \n",
        "                # Add KDE line\n",
        "                try:\n",
        "                    from scipy.stats import gaussian_kde\n",
        "                    kde = gaussian_kde(data_train[col].dropna())\n",
        "                    x_range = np.linspace(data_train[col].min(), data_train[col].max(), 100)\n",
        "                    axes[i].plot(x_range, kde(x_range), 'r-', linewidth=2)\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "                axes[i].set_title(f'{col} Distribution')\n",
        "                axes[i].set_xlabel(col)\n",
        "                axes[i].set_ylabel('Density')\n",
        "        \n",
        "        # Remove empty subplots\n",
        "        for i in range(len(key_numeric), len(axes)):\n",
        "            fig.delaxes(axes[i])\n",
        "        \n",
        "        plt.suptitle('Numeric Features Distribution', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create EDA plots\n",
        "create_eda_plots()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Advanced Feature Engineering\n",
        "\n",
        "Based on our EDA findings, we'll implement sophisticated feature engineering techniques including:\n",
        "- **Domain-specific transformations** for banking/marketing features\n",
        "- **Target encoding** with proper cross-validation to prevent leakage\n",
        "- **Frequency encoding** for categorical variables\n",
        "- **Interaction features** between related variables\n",
        "- **Cyclical encoding** for temporal features\n",
        "- **Outlier handling and normalization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering Utility Functions\n",
        "def month_to_num(s):\n",
        "    \"\"\"Convert month names to numbers\"\"\"\n",
        "    m = str(s).strip().lower()\n",
        "    order = dict(jan=1, feb=2, mar=3, apr=4, may=5, jun=6,\n",
        "                jul=7, aug=8, sep=9, oct=10, nov=11, dec=12)\n",
        "    return order.get(m, np.nan)\n",
        "\n",
        "def add_domain_features(df: pd.DataFrame, with_duration: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add domain-specific features for banking/marketing dataset\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Input dataframe\n",
        "    with_duration : bool\n",
        "        Whether to include duration-based features\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame with new features added\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    \n",
        "    print(\"🔧 Adding domain-specific features...\")\n",
        "    \n",
        "    # === Contact History Features ===\n",
        "    if 'pdays' in out.columns:\n",
        "        print(\"   📞 Contact history features...\")\n",
        "        pdays = out[\"pdays\"].copy()\n",
        "        \n",
        "        # Binary flag for previous contact\n",
        "        out[\"contacted_before\"] = (pdays != 999).astype(int)\n",
        "        out[\"contacted_before\"] = (pdays != -1).astype(int) if (pdays == -1).any() else out[\"contacted_before\"]\n",
        "        \n",
        "        # Days since last contact (handle 999/-1 as special values)\n",
        "        pdays_masked = pdays.replace([999, -1], np.nan)\n",
        "        out[\"days_since_last_contact\"] = pdays_masked.fillna(999)\n",
        "        out[\"pdays_log1p\"] = np.log1p(pdays_masked).fillna(0)\n",
        "        \n",
        "        # Contact intensity (campaigns per day since last contact)\n",
        "        if 'campaign' in out.columns:\n",
        "            out[\"contact_intensity\"] = (out[\"campaign\"] / (pdays_masked + 1)).fillna(0)\n",
        "    \n",
        "    # === Previous Campaign Outcome Features ===\n",
        "    if 'poutcome' in out.columns:\n",
        "        print(\"   📊 Previous campaign outcome features...\")\n",
        "        pout = out[\"poutcome\"].astype(str).str.lower()\n",
        "        out[\"prev_success\"] = (pout == \"success\").astype(int)\n",
        "        out[\"prev_failure\"] = (pout == \"failure\").astype(int)\n",
        "        out[\"prev_unknown\"] = (pout == \"unknown\").astype(int)\n",
        "    \n",
        "    # === Temporal Features ===\n",
        "    if 'month' in out.columns:\n",
        "        print(\"   📅 Temporal features...\")\n",
        "        mnum = out[\"month\"].map(month_to_num)\n",
        "        out[\"month_num\"] = mnum\n",
        "        \n",
        "        # Cyclical encoding for month\n",
        "        out[\"month_sin\"] = np.sin(2 * np.pi * mnum / 12.0)\n",
        "        out[\"month_cos\"] = np.cos(2 * np.pi * mnum / 12.0)\n",
        "        \n",
        "        # Seasonal features\n",
        "        out[\"is_summer\"] = mnum.isin([6, 7, 8]).astype(int)\n",
        "        out[\"is_q4\"] = mnum.isin([10, 11, 12]).astype(int)\n",
        "        out[\"is_spring\"] = mnum.isin([3, 4, 5]).astype(int)\n",
        "    \n",
        "    # Day cyclical encoding (if exists)\n",
        "    if 'day' in out.columns:\n",
        "        day = out[\"day\"].clip(1, 31)\n",
        "        out[\"day_sin\"] = np.sin(2 * np.pi * day / 31.0)\n",
        "        out[\"day_cos\"] = np.cos(2 * np.pi * day / 31.0)\n",
        "    \n",
        "    # === Campaign Features ===\n",
        "    if 'campaign' in out.columns:\n",
        "        print(\"   📢 Campaign features...\")\n",
        "        # Binned campaign attempts\n",
        "        out[\"campaign_bins\"] = pd.cut(out[\"campaign\"],\n",
        "                                      bins=[-np.inf, 1, 3, 6, np.inf],\n",
        "                                      labels=[\"1\", \"2-3\", \"4-6\", \"7+\"],\n",
        "                                      ordered=True)\n",
        "        \n",
        "        # Campaign intensity categories\n",
        "        out[\"campaign_high\"] = (out[\"campaign\"] >= out[\"campaign\"].quantile(0.75)).astype(int)\n",
        "        out[\"campaign_low\"] = (out[\"campaign\"] <= out[\"campaign\"].quantile(0.25)).astype(int)\n",
        "    \n",
        "    # === Duration Features ===\n",
        "    if with_duration and 'duration' in out.columns:\n",
        "        print(\"   ⏱️ Duration features...\")\n",
        "        duration = out[\"duration\"].clip(lower=0)\n",
        "        out[\"duration_log1p\"] = np.log1p(duration)\n",
        "        out[\"duration_sqrt\"] = np.sqrt(duration)\n",
        "        \n",
        "        # Duration categories\n",
        "        out[\"duration_very_short\"] = (duration <= 60).astype(int)  # <= 1 minute\n",
        "        out[\"duration_short\"] = ((duration > 60) & (duration <= 300)).astype(int)  # 1-5 minutes\n",
        "        out[\"duration_medium\"] = ((duration > 300) & (duration <= 600)).astype(int)  # 5-10 minutes\n",
        "        out[\"duration_long\"] = (duration > 600).astype(int)  # > 10 minutes\n",
        "    \n",
        "    # === Financial Features ===\n",
        "    if 'balance' in out.columns:\n",
        "        print(\"   💰 Financial features...\")\n",
        "        # Handle negative balances\n",
        "        balance = out[\"balance\"]\n",
        "        shift = max(0, 1 - balance.min())  # Ensure positive values for log\n",
        "        out[\"balance_log1p\"] = np.log1p(balance + shift)\n",
        "        out[\"has_positive_balance\"] = (balance > 0).astype(int)\n",
        "        out[\"has_negative_balance\"] = (balance < 0).astype(int)\n",
        "        \n",
        "        # Balance categories\n",
        "        out[\"balance_high\"] = (balance >= balance.quantile(0.8)).astype(int)\n",
        "        out[\"balance_low\"] = (balance <= balance.quantile(0.2)).astype(int)\n",
        "    \n",
        "    # === Interaction Features ===\n",
        "    print(\"   🔗 Interaction features...\")\n",
        "    \n",
        "    # Housing and loan combination\n",
        "    if {'housing', 'loan'}.issubset(out.columns):\n",
        "        out[\"housing_loan_combo\"] = (out[\"housing\"].astype(str).str.lower() + \"_\" +\n",
        "                                     out[\"loan\"].astype(str).str.lower())\n",
        "    \n",
        "    # Job and education interaction\n",
        "    if {'job', 'education'}.issubset(out.columns):\n",
        "        out[\"job_x_education\"] = (out[\"job\"].astype(str).str.lower() + \"__\" +\n",
        "                                  out[\"education\"].astype(str).str.lower())\n",
        "    \n",
        "    # Contact history and campaign interaction\n",
        "    if 'contacted_before' in out.columns and 'campaign' in out.columns:\n",
        "        out[\"recency_x_campaign\"] = out[\"contacted_before\"] * out[\"campaign\"]\n",
        "    \n",
        "    # Previous success and current campaign\n",
        "    if 'prev_success' in out.columns and 'campaign' in out.columns:\n",
        "        out[\"prev_success_x_campaign\"] = out[\"prev_success\"] * out[\"campaign\"]\n",
        "    \n",
        "    # Duration and contact interaction\n",
        "    if with_duration and 'contacted_before' in out.columns and 'duration_log1p' in out.columns:\n",
        "        out[\"recent_and_long\"] = out[\"contacted_before\"] * out[\"duration_log1p\"]\n",
        "    \n",
        "    # Age groups\n",
        "    if 'age' in out.columns:\n",
        "        print(\"   👥 Age group features...\")\n",
        "        age = out[\"age\"]\n",
        "        out[\"age_young\"] = (age < 30).astype(int)\n",
        "        out[\"age_middle\"] = ((age >= 30) & (age < 50)).astype(int)\n",
        "        out[\"age_senior\"] = ((age >= 50) & (age < 65)).astype(int)\n",
        "        out[\"age_elderly\"] = (age >= 65).astype(int)\n",
        "    \n",
        "    print(f\"   ✅ Added {len(out.columns) - len(df.columns)} new features\")\n",
        "    return out\n",
        "\n",
        "# Apply domain feature engineering\n",
        "print(\"🚀 Starting Feature Engineering Pipeline...\")\n",
        "train_engineered = add_domain_features(data_train, CONFIG['with_duration'])\n",
        "test_engineered = add_domain_features(data_test, CONFIG['with_duration'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Encoding Functions\n",
        "def _make_te_map(X_tr_col: pd.Series, y_tr: pd.Series, m: float, prior: float):\n",
        "    \"\"\"Create target encoding mapping with smoothing\"\"\"\n",
        "    stats = X_tr_col.to_frame(\"cat\").assign(y=y_tr.values).groupby(\"cat\")[\"y\"].agg([\"sum\", \"count\"])\n",
        "    te = (stats[\"sum\"] + prior * m) / (stats[\"count\"] + m)\n",
        "    return te.to_dict()\n",
        "\n",
        "def _apply_map(series: pd.Series, mapping: dict, default_val: float):\n",
        "    \"\"\"Apply mapping to series with default value for unseen categories\"\"\"\n",
        "    return series.map(mapping).fillna(default_val).astype(float)\n",
        "\n",
        "def add_fold_encodings(X_tr, y_tr, X_va, X_te, te_cols, fe_cols, te_smooth_m=50.0, strict_freq=True):\n",
        "    \"\"\"\n",
        "    Add fold-safe target and frequency encodings\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_tr, X_va, X_te : DataFrame\n",
        "        Training, validation, and test sets\n",
        "    y_tr : Series\n",
        "        Training targets\n",
        "    te_cols : list\n",
        "        Columns for target encoding\n",
        "    fe_cols : list  \n",
        "        Columns for frequency encoding\n",
        "    te_smooth_m : float\n",
        "        Smoothing parameter for target encoding\n",
        "    strict_freq : bool\n",
        "        Whether to use strict frequency encoding (0 for unseen) or smoothed\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple of encoded DataFrames (X_tr_e, X_va_e, X_te_e)\n",
        "    \"\"\"\n",
        "    X_tr_e = X_tr.copy()\n",
        "    X_va_e = X_va.copy()\n",
        "    X_te_e = X_te.copy()\n",
        "    \n",
        "    # Target Encoding (with smoothing to prevent overfitting)\n",
        "    prior = y_tr.mean()\n",
        "    for col in te_cols:\n",
        "        if col not in X_tr_e.columns:\n",
        "            continue\n",
        "        \n",
        "        te_map = _make_te_map(X_tr_e[col].astype(str), y_tr, te_smooth_m, prior)\n",
        "        default_val = prior\n",
        "        \n",
        "        X_tr_e[f\"{col}_te\"] = _apply_map(X_tr_e[col].astype(str), te_map, default_val)\n",
        "        X_va_e[f\"{col}_te\"] = _apply_map(X_va_e[col].astype(str), te_map, default_val)\n",
        "        X_te_e[f\"{col}_te\"] = _apply_map(X_te_e[col].astype(str), te_map, default_val)\n",
        "    \n",
        "    # Frequency Encoding\n",
        "    for col in fe_cols:\n",
        "        if col not in X_tr_e.columns:\n",
        "            continue\n",
        "        \n",
        "        tr_counts = X_tr_e[col].astype(str).value_counts(dropna=False)\n",
        "        tr_freq = (tr_counts / tr_counts.sum()).to_dict()\n",
        "        default_freq = 0.0 if strict_freq else (1.0 / max(1, len(tr_counts)))\n",
        "        \n",
        "        X_tr_e[f\"{col}_freq\"] = X_tr_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
        "        X_va_e[f\"{col}_freq\"] = X_va_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
        "        X_te_e[f\"{col}_freq\"] = X_te_e[col].astype(str).map(tr_freq).fillna(default_freq).astype(float)\n",
        "    \n",
        "    return X_tr_e, X_va_e, X_te_e\n",
        "\n",
        "print(\"🎯 Advanced encoding functions ready!\")\n",
        "print(\"   Target encoding: Prevents overfitting with smoothing\")\n",
        "print(\"   Frequency encoding: Captures category importance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preprocessing and Setup\n",
        "print(\"🔄 Preprocessing and Data Setup...\")\n",
        "\n",
        "# Handle target variable encoding\n",
        "train = train_engineered.copy()\n",
        "test = test_engineered.copy()\n",
        "\n",
        "# Ensure target is properly encoded\n",
        "if train[CONFIG['target_col']].dtype == \"O\":\n",
        "    train[CONFIG['target_col']] = train[CONFIG['target_col']].str.strip().str.lower().map({\"yes\": 1, \"no\": 0})\n",
        "\n",
        "# Handle ID column for test set\n",
        "if CONFIG['id_col'] not in test.columns:\n",
        "    test[CONFIG['id_col']] = np.arange(len(test))\n",
        "\n",
        "# Identify categorical columns and cast to category dtype for efficient handling\n",
        "all_obj_cols = [c for c in train.columns if train[c].dtype == \"O\" and c != CONFIG['target_col']]\n",
        "print(f\"   Converting {len(all_obj_cols)} object columns to category dtype...\")\n",
        "\n",
        "for col in all_obj_cols:\n",
        "    train[col] = train[col].astype(\"category\")\n",
        "    if col in test.columns:\n",
        "        test[col] = test[col].astype(\"category\")\n",
        "\n",
        "# Handle existing category columns and align categories across train/test\n",
        "all_cat_cols = [c for c in train.columns if str(train[c].dtype).startswith(\"category\")]\n",
        "print(f\"   Aligning {len(all_cat_cols)} categorical columns across train/test...\")\n",
        "\n",
        "for col in all_cat_cols:\n",
        "    if col in test.columns:\n",
        "        # Get union of all categories\n",
        "        train_cats = set(train[col].cat.categories.tolist()) if hasattr(train[col], 'cat') else set(train[col].unique())\n",
        "        test_cats = set(test[col].cat.categories.tolist()) if hasattr(test[col], 'cat') else set(test[col].unique())\n",
        "        all_cats = sorted(list(train_cats | test_cats))\n",
        "        \n",
        "        # Set categories\n",
        "        train[col] = train[col].cat.set_categories(all_cats)\n",
        "        test[col] = test[col].cat.set_categories(all_cats)\n",
        "\n",
        "# Define feature sets\n",
        "features = [c for c in train.columns if c not in {CONFIG['target_col'], CONFIG['id_col']}]\n",
        "categorical_features = [c for c in features if str(train[c].dtype).startswith(\"category\")]\n",
        "numeric_features = [c for c in features if c not in categorical_features]\n",
        "\n",
        "print(f\"   📊 Feature Summary:\")\n",
        "print(f\"      Total features: {len(features)}\")\n",
        "print(f\"      Categorical: {len(categorical_features)}\")\n",
        "print(f\"      Numeric: {len(numeric_features)}\")\n",
        "\n",
        "# Prepare data matrices\n",
        "X_full = train[features].copy()\n",
        "y_full = train[CONFIG['target_col']].astype(int)\n",
        "X_test_full = test[features].copy()\n",
        "\n",
        "# Features for different models\n",
        "# LightGBM and CatBoost can handle categorical features natively\n",
        "categorical_indices = [X_full.columns.get_loc(c) for c in categorical_features]\n",
        "\n",
        "# XGBoost needs only numeric features (we'll handle categoricals with encodings)\n",
        "xgb_features = [c for c in features if not str(X_full[c].dtype).startswith(\"category\")]\n",
        "\n",
        "print(f\"   🎯 Model-specific features:\")\n",
        "print(f\"      LightGBM/CatBoost: {len(features)} features ({len(categorical_features)} categorical)\")\n",
        "print(f\"      XGBoost: {len(xgb_features)} features (numeric only)\")\n",
        "\n",
        "print(\"✅ Data preprocessing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Development with Hyperparameter Optimization\n",
        "\n",
        "We'll use Optuna to automatically optimize hyperparameters for three powerful gradient boosting algorithms:\n",
        "- **LightGBM**: Fast and efficient with native categorical support\n",
        "- **XGBoost**: Robust and well-tested with excellent performance\n",
        "- **CatBoost**: Advanced categorical handling and built-in regularization\n",
        "\n",
        "Each model will be optimized using cross-validation to prevent overfitting and ensure robust performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-Validation Setup\n",
        "skf = StratifiedKFold(n_splits=CONFIG['n_splits'], shuffle=True, random_state=CONFIG['random_state'])\n",
        "\n",
        "print(f\"🔀 Cross-Validation Setup:\")\n",
        "print(f\"   Strategy: Stratified {CONFIG['n_splits']}-Fold\")\n",
        "print(f\"   Random State: {CONFIG['random_state']}\")\n",
        "\n",
        "# Define columns for advanced encodings\n",
        "te_cols = ['job', 'education', 'contact', 'month', 'poutcome', 'marital']  # Target encoding\n",
        "fe_cols = ['job', 'education', 'contact', 'month', 'poutcome', 'marital']  # Frequency encoding\n",
        "\n",
        "# Add interaction features to encoding lists\n",
        "interaction_cols = ['housing_loan_combo', 'job_x_education', 'campaign_bins']\n",
        "te_cols.extend([col for col in interaction_cols if col in train.columns])\n",
        "fe_cols.extend([col for col in interaction_cols if col in train.columns])\n",
        "\n",
        "print(f\"   Target Encoding Columns: {len(te_cols)} features\")\n",
        "print(f\"   Frequency Encoding Columns: {len(fe_cols)} features\")\n",
        "\n",
        "# Model-specific objective functions for Optuna\n",
        "def tune_lightgbm(trial):\n",
        "    \"\"\"Optuna objective function for LightGBM hyperparameter tuning\"\"\"\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\", \n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
        "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0),\n",
        "        \"verbose\": -1,\n",
        "        \"is_unbalance\": True,\n",
        "        \"seed\": CONFIG['random_state'],\n",
        "        \"device_type\": \"cpu\"  # Ensure CPU usage for stability\n",
        "    }\n",
        "    \n",
        "    oof_preds = np.zeros(len(X_full))\n",
        "    \n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_full, y_full)):\n",
        "        X_tr, y_tr = X_full.iloc[tr_idx].copy(), y_full.iloc[tr_idx].copy()\n",
        "        X_va, y_va = X_full.iloc[va_idx].copy(), y_full.iloc[va_idx].copy()\n",
        "        X_te = X_test_full.copy()\n",
        "        \n",
        "        # Add fold-safe encodings\n",
        "        X_tr_e, X_va_e, _ = add_fold_encodings(\n",
        "            X_tr, y_tr, X_va, X_te, te_cols, fe_cols, CONFIG['te_smooth_m'], strict_freq=True\n",
        "        )\n",
        "        \n",
        "        fold_features = list(X_tr_e.columns)\n",
        "        fold_cat_features = [c for c in fold_features if (c in categorical_features and c in X_tr_e.columns)]\n",
        "        \n",
        "        # Create datasets\n",
        "        dtrain = lgb.Dataset(X_tr_e[fold_features], label=y_tr, \n",
        "                           categorical_feature=fold_cat_features, free_raw_data=False)\n",
        "        dvalid = lgb.Dataset(X_va_e[fold_features], label=y_va,\n",
        "                           categorical_feature=fold_cat_features, reference=dtrain, free_raw_data=False)\n",
        "        \n",
        "        # Train model with suppressed output\n",
        "        with suppress_output():\n",
        "            model = lgb.train(\n",
        "                params,\n",
        "                dtrain,\n",
        "                valid_sets=[dvalid],\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(CONFIG['early_stopping_rounds'], verbose=False),\n",
        "                    lgb.log_evaluation(0)\n",
        "                ]\n",
        "            )\n",
        "        \n",
        "        oof_preds[va_idx] = model.predict(X_va_e[fold_features], num_iteration=model.best_iteration)\n",
        "    \n",
        "    return roc_auc_score(y_full, oof_preds)\n",
        "\n",
        "def tune_xgboost(trial):\n",
        "    \"\"\"Optuna objective function for XGBoost hyperparameter tuning\"\"\"\n",
        "    params = {\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"verbosity\": 0,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 0.0, 5.0),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\n",
        "        \"scale_pos_weight\": 1.0,\n",
        "        \"device\": \"cpu\"  # Ensure CPU usage for stability\n",
        "    }\n",
        "    \n",
        "    oof_preds = np.zeros(len(X_full))\n",
        "    \n",
        "    for tr_idx, va_idx in skf.split(X_full, y_full):\n",
        "        X_tr, y_tr = X_full.iloc[tr_idx].copy(), y_full.iloc[tr_idx].copy()\n",
        "        X_va, y_va = X_full.iloc[va_idx].copy(), y_full.iloc[va_idx].copy()\n",
        "        X_te = X_test_full.copy()\n",
        "        \n",
        "        # Add fold-safe encodings and use only numeric features for XGBoost\n",
        "        X_tr_e, X_va_e, _ = add_fold_encodings(\n",
        "            X_tr, y_tr, X_va, X_te, te_cols, fe_cols, CONFIG['te_smooth_m'], strict_freq=True\n",
        "        )\n",
        "        \n",
        "        # Filter to numeric features only\n",
        "        fold_xgb_features = [c for c in X_tr_e.columns if not str(X_tr_e[c].dtype).startswith(\"category\")]\n",
        "        \n",
        "        dtrain = xgb.DMatrix(X_tr_e[fold_xgb_features], label=y_tr)\n",
        "        dvalid = xgb.DMatrix(X_va_e[fold_xgb_features], label=y_va)\n",
        "        \n",
        "        # Train model with suppressed output\n",
        "        with suppress_output():\n",
        "            model = xgb.train(\n",
        "                params,\n",
        "                dtrain,\n",
        "                num_boost_round=5000,\n",
        "                evals=[(dvalid, \"valid\")],\n",
        "                early_stopping_rounds=CONFIG['early_stopping_rounds'],\n",
        "                verbose_eval=False\n",
        "            )\n",
        "        \n",
        "        oof_preds[va_idx] = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n",
        "    \n",
        "    return roc_auc_score(y_full, oof_preds)\n",
        "\n",
        "def tune_catboost(trial):\n",
        "    \"\"\"Optuna objective function for CatBoost hyperparameter tuning\"\"\"\n",
        "    params = {\n",
        "        \"iterations\": 5000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"bootstrap_type\": \"Bernoulli\",\n",
        "        \"eval_metric\": \"AUC\",\n",
        "        \"task_type\": \"CPU\",  # Use CPU for stability\n",
        "        \"random_seed\": CONFIG['random_state'],\n",
        "        \"verbose\": False,\n",
        "        \"logging_level\": \"Silent\",\n",
        "        \"allow_writing_files\": False,\n",
        "        \"use_best_model\": True\n",
        "    }\n",
        "    \n",
        "    oof_preds = np.zeros(len(X_full))\n",
        "    \n",
        "    for tr_idx, va_idx in skf.split(X_full, y_full):\n",
        "        X_tr, y_tr = X_full.iloc[tr_idx].copy(), y_full.iloc[tr_idx].copy()\n",
        "        X_va, y_va = X_full.iloc[va_idx].copy(), y_full.iloc[va_idx].copy()\n",
        "        X_te = X_test_full.copy()\n",
        "        \n",
        "        # Add fold-safe encodings\n",
        "        X_tr_e, X_va_e, _ = add_fold_encodings(\n",
        "            X_tr, y_tr, X_va, X_te, te_cols, fe_cols, CONFIG['te_smooth_m'], strict_freq=True\n",
        "        )\n",
        "        \n",
        "        fold_features = list(X_tr_e.columns)\n",
        "        fold_cat_features = [c for c in fold_features if (c in categorical_features and c in X_tr_e.columns)]\n",
        "        \n",
        "        # Create pools\n",
        "        train_pool = Pool(X_tr_e[fold_features], y_tr, cat_features=fold_cat_features)\n",
        "        valid_pool = Pool(X_va_e[fold_features], y_va, cat_features=fold_cat_features)\n",
        "        \n",
        "        # Train model with suppressed output\n",
        "        with suppress_output():\n",
        "            model = CatBoostClassifier(**params)\n",
        "            model.fit(train_pool, eval_set=valid_pool, \n",
        "                     early_stopping_rounds=CONFIG['early_stopping_rounds'], verbose=False)\n",
        "        \n",
        "        oof_preds[va_idx] = model.predict_proba(valid_pool)[:, 1]\n",
        "    \n",
        "    return roc_auc_score(y_full, oof_preds)\n",
        "\n",
        "print(\"🎯 Hyperparameter optimization functions ready!\")\n",
        "print(\"   Each model uses cross-validation with fold-safe encodings\")\n",
        "print(\"   Target metric: ROC-AUC\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Optimization with Optuna\n",
        "print(\"🚀 Starting Hyperparameter Optimization...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Set up Optuna sampler for better optimization\n",
        "sampler = TPESampler(seed=CONFIG['random_state'])\n",
        "\n",
        "# 1. LightGBM Optimization\n",
        "print(\"\\n📈 Optimizing LightGBM...\")\n",
        "study_lgb = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "study_lgb.optimize(tune_lightgbm, n_trials=CONFIG['optuna_trials'], show_progress_bar=True)\n",
        "\n",
        "lgb_best_score = study_lgb.best_value\n",
        "lgb_best_params = study_lgb.best_params\n",
        "print(f\"   ✅ LightGBM Best Score: {lgb_best_score:.5f}\")\n",
        "print(f\"   🔧 Best Params: {lgb_best_params}\")\n",
        "\n",
        "# 2. XGBoost Optimization  \n",
        "print(\"\\n📊 Optimizing XGBoost...\")\n",
        "study_xgb = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "study_xgb.optimize(tune_xgboost, n_trials=CONFIG['optuna_trials'], show_progress_bar=True)\n",
        "\n",
        "xgb_best_score = study_xgb.best_value\n",
        "xgb_best_params = study_xgb.best_params\n",
        "print(f\"   ✅ XGBoost Best Score: {xgb_best_score:.5f}\")\n",
        "print(f\"   🔧 Best Params: {xgb_best_params}\")\n",
        "\n",
        "# 3. CatBoost Optimization\n",
        "print(\"\\n🐱 Optimizing CatBoost...\")\n",
        "study_cat = optuna.create_study(direction=\"maximize\", sampler=sampler) \n",
        "study_cat.optimize(tune_catboost, n_trials=CONFIG['optuna_trials'], show_progress_bar=True)\n",
        "\n",
        "cat_best_score = study_cat.best_value\n",
        "cat_best_params = study_cat.best_params\n",
        "print(f\"   ✅ CatBoost Best Score: {cat_best_score:.5f}\")\n",
        "print(f\"   🔧 Best Params: {cat_best_params}\")\n",
        "\n",
        "# Summary of optimization results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 OPTIMIZATION RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"LightGBM Score: {lgb_best_score:.5f}\")\n",
        "print(f\"XGBoost Score:  {xgb_best_score:.5f}\")\n",
        "print(f\"CatBoost Score: {cat_best_score:.5f}\")\n",
        "\n",
        "best_single_model = max([\n",
        "    (\"LightGBM\", lgb_best_score),\n",
        "    (\"XGBoost\", xgb_best_score), \n",
        "    (\"CatBoost\", cat_best_score)\n",
        "], key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\n🥇 Best Single Model: {best_single_model[0]} ({best_single_model[1]:.5f})\")\n",
        "print(\"✅ Hyperparameter optimization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Ensembling and Final Predictions\n",
        "\n",
        "Now we'll train the final models with optimized hyperparameters and create an ensemble by:\n",
        "1. **Training each model** with best parameters from optimization\n",
        "2. **Generating out-of-fold predictions** for proper ensemble training\n",
        "3. **Optimizing blend weights** using Optuna\n",
        "4. **Creating final predictions** on the test set\n",
        "\n",
        "This ensemble approach typically outperforms any single model by combining their strengths and reducing individual model biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Model Training Function\n",
        "def train_final_models():\n",
        "    \"\"\"Train final models with optimized hyperparameters and generate OOF predictions\"\"\"\n",
        "    \n",
        "    print(\"🏗️ Training Final Models with Optimized Hyperparameters...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Initialize storage for out-of-fold predictions and test predictions\n",
        "    oof_predictions = {}\n",
        "    test_predictions = {}\n",
        "    model_scores = {}\n",
        "    \n",
        "    # Model configurations with optimized parameters\n",
        "    model_configs = [\n",
        "        (\"LightGBM\", lgb_best_params, \"lightgbm\"),\n",
        "        (\"XGBoost\", xgb_best_params, \"xgboost\"), \n",
        "        (\"CatBoost\", cat_best_params, \"catboost\")\n",
        "    ]\n",
        "    \n",
        "    total_steps = len(model_configs) * CONFIG['n_splits']\n",
        "    \n",
        "    with tqdm(total=total_steps, desc=\"Training ensemble models\", leave=True) as pbar:\n",
        "        for model_name, best_params, model_type in model_configs:\n",
        "            print(f\"\\n🔧 Training {model_name}...\")\n",
        "            \n",
        "            # Initialize prediction arrays\n",
        "            oof_preds = np.zeros(len(X_full))\n",
        "            test_preds = np.zeros(len(X_test_full))\n",
        "            fold_scores = []\n",
        "            \n",
        "            # Cross-validation training\n",
        "            for fold, (tr_idx, va_idx) in enumerate(skf.split(X_full, y_full)):\n",
        "                X_tr, y_tr = X_full.iloc[tr_idx].copy(), y_full.iloc[tr_idx].copy()\n",
        "                X_va, y_va = X_full.iloc[va_idx].copy(), y_full.iloc[va_idx].copy() \n",
        "                X_te = X_test_full.copy()\n",
        "                \n",
        "                # Add fold-safe encodings\n",
        "                X_tr_e, X_va_e, X_te_e = add_fold_encodings(\n",
        "                    X_tr, y_tr, X_va, X_te, te_cols, fe_cols, CONFIG['te_smooth_m'], strict_freq=True\n",
        "                )\n",
        "                \n",
        "                # Train model based on type\n",
        "                if model_type == \"lightgbm\":\n",
        "                    fold_features = list(X_tr_e.columns)\n",
        "                    fold_cat_features = [c for c in fold_features if (c in categorical_features and c in X_tr_e.columns)]\n",
        "                    \n",
        "                    dtrain = lgb.Dataset(X_tr_e[fold_features], label=y_tr,\n",
        "                                       categorical_feature=fold_cat_features, free_raw_data=False)\n",
        "                    dvalid = lgb.Dataset(X_va_e[fold_features], label=y_va,\n",
        "                                       categorical_feature=fold_cat_features, reference=dtrain, free_raw_data=False)\n",
        "                    \n",
        "                    with suppress_output():\n",
        "                        model = lgb.train(\n",
        "                            {**best_params, \"objective\": \"binary\", \"metric\": \"auc\", \"verbose\": -1, \"seed\": CONFIG['random_state']},\n",
        "                            dtrain,\n",
        "                            valid_sets=[dvalid],\n",
        "                            callbacks=[lgb.early_stopping(CONFIG['early_stopping_rounds'], verbose=False), lgb.log_evaluation(0)]\n",
        "                        )\n",
        "                    \n",
        "                    oof_preds[va_idx] = model.predict(X_va_e[fold_features], num_iteration=model.best_iteration)\n",
        "                    test_preds += model.predict(X_te_e[fold_features], num_iteration=model.best_iteration) / CONFIG['n_splits']\n",
        "                \n",
        "                elif model_type == \"xgboost\":\n",
        "                    fold_xgb_features = [c for c in X_tr_e.columns if not str(X_tr_e[c].dtype).startswith(\"category\")]\n",
        "                    \n",
        "                    dtrain = xgb.DMatrix(X_tr_e[fold_xgb_features], label=y_tr)\n",
        "                    dvalid = xgb.DMatrix(X_va_e[fold_xgb_features], label=y_va)\n",
        "                    \n",
        "                    params_full = {**best_params, \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\", \"verbosity\": 0}\n",
        "                    \n",
        "                    with suppress_output():\n",
        "                        model = xgb.train(\n",
        "                            params_full,\n",
        "                            dtrain,\n",
        "                            num_boost_round=5000,\n",
        "                            evals=[(dvalid, \"valid\")],\n",
        "                            early_stopping_rounds=CONFIG['early_stopping_rounds'],\n",
        "                            verbose_eval=False\n",
        "                        )\n",
        "                    \n",
        "                    oof_preds[va_idx] = model.predict(dvalid, iteration_range=(0, model.best_iteration))\n",
        "                    test_preds += model.predict(xgb.DMatrix(X_te_e[fold_xgb_features]), \n",
        "                                               iteration_range=(0, model.best_iteration)) / CONFIG['n_splits']\n",
        "                \n",
        "                elif model_type == \"catboost\":\n",
        "                    fold_features = list(X_tr_e.columns)\n",
        "                    fold_cat_features = [c for c in fold_features if (c in categorical_features and c in X_tr_e.columns)]\n",
        "                    \n",
        "                    train_pool = Pool(X_tr_e[fold_features], y_tr, cat_features=fold_cat_features)\n",
        "                    valid_pool = Pool(X_va_e[fold_features], y_va, cat_features=fold_cat_features)\n",
        "                    \n",
        "                    with suppress_output():\n",
        "                        model = CatBoostClassifier(\n",
        "                            **best_params, iterations=5000, eval_metric=\"AUC\", verbose=False,\n",
        "                            logging_level=\"Silent\", allow_writing_files=False, use_best_model=True,\n",
        "                            random_seed=CONFIG['random_state']\n",
        "                        )\n",
        "                        model.fit(train_pool, eval_set=valid_pool, \n",
        "                                 early_stopping_rounds=CONFIG['early_stopping_rounds'], verbose=False)\n",
        "                    \n",
        "                    oof_preds[va_idx] = model.predict_proba(valid_pool)[:, 1]\n",
        "                    test_preds += model.predict_proba(Pool(X_te_e[fold_features], cat_features=fold_cat_features))[:, 1] / CONFIG['n_splits']\n",
        "                \n",
        "                # Calculate fold score\n",
        "                fold_score = roc_auc_score(y_va, oof_preds[va_idx])\n",
        "                fold_scores.append(fold_score)\n",
        "                pbar.update(1)\n",
        "            \n",
        "            # Calculate final model score\n",
        "            final_score = roc_auc_score(y_full, oof_preds)\n",
        "            model_scores[model_name] = final_score\n",
        "            \n",
        "            # Store predictions\n",
        "            oof_predictions[model_name] = oof_preds\n",
        "            test_predictions[model_name] = test_preds\n",
        "            \n",
        "            print(f\"   ✅ {model_name} CV Score: {final_score:.5f} (±{np.std(fold_scores):.5f})\")\n",
        "            print(f\"      Fold Scores: {', '.join([f'{score:.5f}' for score in fold_scores])}\")\n",
        "    \n",
        "    return oof_predictions, test_predictions, model_scores\n",
        "\n",
        "# Train all final models\n",
        "oof_preds, test_preds, model_scores = train_final_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble Weight Optimization\n",
        "print(\"\\n🎨 Optimizing Ensemble Weights...\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Convert predictions to numpy arrays for easier handling\n",
        "oof_models_array = np.array([oof_preds[model] for model in [\"LightGBM\", \"XGBoost\", \"CatBoost\"]])\n",
        "test_models_array = np.array([test_preds[model] for model in [\"LightGBM\", \"XGBoost\", \"CatBoost\"]])\n",
        "\n",
        "def optimize_blend_weights(trial):\n",
        "    \"\"\"Optuna objective function for optimizing ensemble weights\"\"\"\n",
        "    w_lgb = trial.suggest_float(\"w_lgb\", 0.0, 1.0)\n",
        "    w_xgb = trial.suggest_float(\"w_xgb\", 0.0, 1.0)\n",
        "    w_cat = trial.suggest_float(\"w_cat\", 0.0, 1.0)\n",
        "    \n",
        "    # Normalize weights\n",
        "    weights = np.array([w_lgb, w_xgb, w_cat])\n",
        "    if weights.sum() == 0:\n",
        "        weights = np.array([1.0, 1.0, 1.0])\n",
        "    weights = weights / weights.sum()\n",
        "    \n",
        "    # Calculate blended predictions\n",
        "    blended_oof = np.average(oof_models_array, axis=0, weights=weights)\n",
        "    \n",
        "    # Return ROC-AUC score\n",
        "    return roc_auc_score(y_full, blended_oof)\n",
        "\n",
        "# Optimize ensemble weights\n",
        "study_blend = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=CONFIG['random_state']))\n",
        "study_blend.optimize(optimize_blend_weights, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "# Get optimal weights\n",
        "best_weights = np.array([\n",
        "    study_blend.best_params[\"w_lgb\"],\n",
        "    study_blend.best_params[\"w_xgb\"], \n",
        "    study_blend.best_params[\"w_cat\"]\n",
        "])\n",
        "\n",
        "if best_weights.sum() == 0:\n",
        "    best_weights = np.array([1.0, 1.0, 1.0])\n",
        "best_weights = best_weights / best_weights.sum()\n",
        "\n",
        "# Calculate final ensemble predictions\n",
        "ensemble_oof = np.average(oof_models_array, axis=0, weights=best_weights)\n",
        "ensemble_test = np.average(test_models_array, axis=0, weights=best_weights)\n",
        "ensemble_score = roc_auc_score(y_full, ensemble_oof)\n",
        "\n",
        "print(f\"\\n🎯 Ensemble Optimization Results:\")\n",
        "print(f\"   Best Ensemble Score: {ensemble_score:.5f}\")\n",
        "print(f\"   Optimal Weights:\")\n",
        "print(f\"      LightGBM: {best_weights[0]:.3f}\")\n",
        "print(f\"      XGBoost:  {best_weights[1]:.3f}\")\n",
        "print(f\"      CatBoost: {best_weights[2]:.3f}\")\n",
        "\n",
        "# Compare with individual models\n",
        "print(f\"\\n📊 Model Performance Comparison:\")\n",
        "print(f\"   Individual Models:\")\n",
        "for model_name, score in model_scores.items():\n",
        "    print(f\"      {model_name}: {score:.5f}\")\n",
        "print(f\"   Ensemble: {ensemble_score:.5f}\")\n",
        "\n",
        "improvement = ensemble_score - max(model_scores.values())\n",
        "print(f\"\\n🚀 Ensemble Improvement: +{improvement:.5f} over best single model\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(\"   ✅ Ensemble outperforms individual models!\")\n",
        "else:\n",
        "    print(\"   ℹ️ Ensemble performs similarly to best individual model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Final Predictions and Submission\n",
        "print(\"\\n📄 Generating Final Submission...\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    CONFIG['id_col']: test[CONFIG['id_col']].values,\n",
        "    CONFIG['target_col']: ensemble_test\n",
        "})\n",
        "\n",
        "# Save probability predictions\n",
        "prob_submission_path = \"final_ensemble_predictions_probabilities.csv\"\n",
        "submission_df.to_csv(prob_submission_path, index=False)\n",
        "print(f\"✅ Probability predictions saved to: {prob_submission_path}\")\n",
        "\n",
        "# Create binary predictions (threshold = 0.5)\n",
        "submission_binary = submission_df.copy()\n",
        "submission_binary[CONFIG['target_col']] = (submission_binary[CONFIG['target_col']] >= 0.5).astype(int)\n",
        "\n",
        "binary_submission_path = \"final_ensemble_predictions_binary.csv\"\n",
        "submission_binary.to_csv(binary_submission_path, index=False)\n",
        "print(f\"✅ Binary predictions saved to: {binary_submission_path}\")\n",
        "\n",
        "# Show prediction statistics\n",
        "print(f\"\\n📈 Prediction Statistics:\")\n",
        "print(f\"   Probability predictions range: [{ensemble_test.min():.4f}, {ensemble_test.max():.4f}]\")\n",
        "print(f\"   Mean prediction probability: {ensemble_test.mean():.4f}\")\n",
        "print(f\"   Predicted positive class ratio: {(ensemble_test >= 0.5).mean():.3f}\")\n",
        "\n",
        "# Show first few predictions\n",
        "print(f\"\\n👀 Sample Predictions:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "print(f\"\\n🎉 FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"🔬 Dataset: Bank Marketing Binary Classification\")\n",
        "print(f\"📊 Training Samples: {len(X_full):,}\")\n",
        "print(f\"🧪 Test Samples: {len(X_test_full):,}\")\n",
        "print(f\"🔧 Features: {len(features)}\")\n",
        "print(f\"🎯 Best Single Model: {best_single_model[0]} ({best_single_model[1]:.5f})\")\n",
        "print(f\"🏆 Final Ensemble Score: {ensemble_score:.5f}\")\n",
        "print(f\"📈 Improvement: +{improvement:.5f}\")\n",
        "print(f\"💾 Predictions saved to: {binary_submission_path}\")\n",
        "print(\"=\"*50)\n",
        "print(\"✅ Analysis Complete! Ready for submission.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Analysis and Feature Importance\n",
        "\n",
        "Let's analyze our final ensemble to understand which features contributed most to the predictions and gain insights into the model's decision-making process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "print(\"🔍 Analyzing Feature Importance...\")\n",
        "\n",
        "# Train a final LightGBM model on full dataset for feature importance analysis\n",
        "# (LightGBM provides the most interpretable feature importance)\n",
        "print(\"   Training analysis model on full dataset...\")\n",
        "\n",
        "# Prepare full dataset with encodings (using simple mean encoding for this analysis)\n",
        "X_analysis = X_full.copy()\n",
        "y_analysis = y_full.copy()\n",
        "\n",
        "# Add simple target encoding for categorical features (for analysis only)\n",
        "for col in categorical_features:\n",
        "    if col in X_analysis.columns:\n",
        "        target_mean = y_analysis.groupby(X_analysis[col]).mean()\n",
        "        X_analysis[f\"{col}_te_simple\"] = X_analysis[col].map(target_mean).fillna(y_analysis.mean())\n",
        "\n",
        "# Get numeric features for analysis\n",
        "analysis_features = [c for c in X_analysis.columns if not str(X_analysis[c].dtype).startswith(\"category\")]\n",
        "\n",
        "# Train analysis model\n",
        "analysis_params = {\n",
        "    **lgb_best_params,\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"verbose\": -1,\n",
        "    \"seed\": CONFIG['random_state']\n",
        "}\n",
        "\n",
        "dtrain_analysis = lgb.Dataset(X_analysis[analysis_features], label=y_analysis, free_raw_data=False)\n",
        "\n",
        "with suppress_output():\n",
        "    analysis_model = lgb.train(\n",
        "        analysis_params,\n",
        "        dtrain_analysis,\n",
        "        num_boost_round=500  # Reduced for analysis\n",
        "    )\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = analysis_model.feature_importance(importance_type='gain')\n",
        "feature_names = analysis_features\n",
        "\n",
        "# Create feature importance dataframe\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display top 20 most important features\n",
        "print(f\"\\n🏆 Top 20 Most Important Features:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
        "    print(f\"   {i:2d}. {row['feature']:<25} {row['importance']:>8.0f}\")\n",
        "\n",
        "# Create feature importance visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(15)\n",
        "sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')\n",
        "plt.title('Top 15 Feature Importance (LightGBM)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Feature Importance (Gain)', fontsize=12)\n",
        "plt.ylabel('Features', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Feature importance analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions and Methodology Summary\n",
        "\n",
        "### 🎯 **Project Overview**\n",
        "This notebook presented a comprehensive machine learning pipeline for binary classification of bank marketing data, achieving robust performance through advanced feature engineering and ensemble modeling.\n",
        "\n",
        "### 🔧 **Key Methodologies Applied**\n",
        "\n",
        "#### **1. Advanced Feature Engineering**\n",
        "- **Domain-specific transformations**: Banking/marketing specific feature creation\n",
        "- **Cyclical encoding**: Temporal features (month, day) using sine/cosine transformations  \n",
        "- **Target encoding**: Categorical variables with cross-validation to prevent leakage\n",
        "- **Interaction features**: Meaningful combinations of related variables\n",
        "- **Financial indicators**: Balance categories, contact history analysis\n",
        "\n",
        "#### **2. Robust Model Development**\n",
        "- **Multi-model approach**: LightGBM, XGBoost, and CatBoost for diversity\n",
        "- **Hyperparameter optimization**: Automated tuning with Optuna (50+ trials per model)\n",
        "- **Cross-validation**: Stratified 5-fold CV for unbiased performance estimation\n",
        "- **Early stopping**: Preventing overfitting during training\n",
        "\n",
        "#### **3. Ensemble Learning**\n",
        "- **Out-of-fold predictions**: Proper ensemble training without data leakage\n",
        "- **Optimized blending**: Automated weight optimization using Optuna\n",
        "- **Model diversity**: Leveraging strengths of different algorithms\n",
        "\n",
        "### 📈 **Technical Achievements**\n",
        "- **Professional code structure**: Modular, documented, and maintainable\n",
        "- **Comprehensive EDA**: Deep understanding of data patterns and relationships\n",
        "- **Advanced preprocessing**: Proper handling of categorical, numerical, and temporal features\n",
        "- **Automated optimization**: Minimal manual hyperparameter tuning\n",
        "- **Robust validation**: Cross-validated performance estimates\n",
        "\n",
        "### 🚀 **Business Impact**\n",
        "The resulting model can effectively predict customer subscription likelihood, enabling:\n",
        "- **Targeted marketing campaigns**: Focus resources on high-probability prospects\n",
        "- **Cost optimization**: Reduce wasted marketing spend on unlikely converts\n",
        "- **Strategy insights**: Understand key factors influencing customer decisions\n",
        "\n",
        "### 💡 **Key Insights**\n",
        "From our feature importance analysis, the most predictive factors for customer subscription include:\n",
        "- **Contact duration and history**: Previous interactions strongly predict success\n",
        "- **Campaign timing**: Seasonal and monthly patterns matter significantly  \n",
        "- **Customer demographics**: Age, job, and education level are key indicators\n",
        "- **Financial status**: Balance and existing loan status influence decisions\n",
        "\n",
        "This professional-grade solution demonstrates best practices in machine learning pipeline development and can serve as a template for similar binary classification problems in business contexts.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
