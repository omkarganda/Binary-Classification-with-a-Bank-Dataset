{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler  # or StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.drop([\"id\"], axis =1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e609b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate target\n",
    "y = df[\"y\"]\n",
    "X = df.drop(columns=[\"y\"])\n",
    "\n",
    "# One-hot encode\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "X_enc = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Identify continuous numeric columns from the pre-encoded data\n",
    "# (more reliable if you pick them before encoding; otherwise use a heuristic)\n",
    "cont_num_cols = [\n",
    "    c for c in X.columns\n",
    "    if pd.api.types.is_numeric_dtype(X[c]) and X[c].nunique() > 2\n",
    "]\n",
    "\n",
    "# Scale only those continuous columns\n",
    "scaler = RobustScaler()  # alt: StandardScaler() or MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = X_enc.copy()\n",
    "X_scaled[cont_num_cols] = scaler.fit_transform(X_scaled[cont_num_cols])\n",
    "X_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa984fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# --- Assumptions ---\n",
    "# You already have:\n",
    "#   y            -> target (0/1)\n",
    "#   X_scaled     -> your scaled + one-hot encoded features\n",
    "# If your variable is named differently (e.g., X_encoded or X_processed), swap it in.\n",
    "\n",
    "X = X_scaled  # or X_processed / X_encoded\n",
    "y = y         # keep as is\n",
    "\n",
    "# Train/valid split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost likes DMatrix for speed and memory efficiency\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "# Choose CPU or GPU tree method\n",
    "use_gpu = False  # set True if you have a CUDA-capable GPU + xgboost built with CUDA\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"auc\"],\n",
    "    \"eta\": 0.05,                 # learning_rate\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n",
    "    \"nthread\": -1,\n",
    "}\n",
    "\n",
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 50\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "# --- tqdm callback for pretty progress ---\n",
    "class TQDMCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, total):\n",
    "        self.total = total\n",
    "        self.pbar = None\n",
    "\n",
    "    def after_training(self, model):\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.close()\n",
    "        return model\n",
    "\n",
    "    def before_training(self, model):\n",
    "        self.pbar = tqdm(total=self.total, desc=\"XGBoost training\", leave=True)\n",
    "        return model\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        # Update bar and show latest valid metrics in postfix\n",
    "        self.pbar.update(1)\n",
    "\n",
    "        # Pull last metric values for display\n",
    "        try:\n",
    "            # evals_log looks like:\n",
    "            # {'train': {'logloss': [...], 'auc': [...]}, 'valid': {'logloss': [...], 'auc': [...]} }\n",
    "            last_train = {m: vals[-1] for m, vals in evals_log[\"train\"].items()}\n",
    "            last_valid = {m: vals[-1] for m, vals in evals_log[\"valid\"].items()}\n",
    "            self.pbar.set_postfix({\n",
    "                \"train_logloss\": f\"{last_train.get('logloss', np.nan):.4f}\",\n",
    "                \"valid_logloss\": f\"{last_valid.get('logloss', np.nan):.4f}\",\n",
    "                \"valid_auc\":     f\"{last_valid.get('auc', np.nan):.4f}\",\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Return False to continue training\n",
    "        return False\n",
    "\n",
    "# Early stopping callback (saves best score/iteration)\n",
    "es_cb = xgb.callback.EarlyStopping(\n",
    "    rounds=early_stopping_rounds,\n",
    "    save_best=True,          # keep best iteration\n",
    "    maximize=True,           # because 'auc' is a maximize metric\n",
    "    data_name=\"valid\",\n",
    "    metric_name=\"auc\"\n",
    ")\n",
    "\n",
    "# Train\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    callbacks=[TQDMCallback(num_boost_round), es_cb],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7311f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best iteration: 996  (used trees: 997)\n",
      "Valid AUC: 0.9673 | Valid Acc: 0.9347\n"
     ]
    }
   ],
   "source": [
    "# Inference at best iteration (XGBoost >= 1.6/2.0 style)\n",
    "best_iter = getattr(bst, \"best_iteration\", None)\n",
    "if best_iter is None:\n",
    "    # Fallback if early stopping didn't set it\n",
    "    best_iter = bst.num_boosted_rounds() - 1  # zero-based\n",
    "\n",
    "# Use iteration_range instead of ntree_limit\n",
    "y_prob = bst.predict(dvalid, iteration_range=(0, best_iter + 1))\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "auc = roc_auc_score(y_valid, y_prob)\n",
    "acc = accuracy_score(y_valid, y_pred)\n",
    "print(f\"\\nBest iteration: {best_iter}  (used trees: {best_iter + 1})\")\n",
    "print(f\"Valid AUC: {auc:.4f} | Valid Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1840202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id    y_prob  y_pred\n",
      "0  750000  0.001079       0\n",
      "1  750001  0.121239       0\n",
      "2  750002  0.000277       0\n",
      "3  750003  0.000141       0\n",
      "4  750004  0.019914       0\n",
      "Saved predictions to xgb_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# --- 1) Load test data ---\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "if \"id\" in test_df.columns:\n",
    "    ids = test_df[\"id\"].copy()\n",
    "    test_df = test_df.drop(columns=[\"id\"])\n",
    "else:\n",
    "    ids = pd.Series(range(len(test_df)), name=\"id\")\n",
    "\n",
    "# --- 2) One-hot encode categorical columns with the SAME setup as train ---\n",
    "# IMPORTANT: use the same `cat_cols` and drop_first=True choice you used on train\n",
    "test_enc = pd.get_dummies(test_df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Align to training matrix columns (adds any missing OHE columns with zeros and orders columns identically)\n",
    "test_enc = test_enc.reindex(columns=X_scaled.columns, fill_value=0)\n",
    "\n",
    "# --- 3) Scale ONLY the continuous numeric columns with the FITTED RobustScaler ---\n",
    "# These columns exist in test_enc unchanged (since only categorical were OHE'ed)\n",
    "test_enc[cont_num_cols] = scaler.transform(test_enc[cont_num_cols])\n",
    "\n",
    "# --- 4) Predict with the best iteration ---\n",
    "dtest = xgb.DMatrix(test_enc)\n",
    "best_iter = getattr(bst, \"best_iteration\", None)\n",
    "if best_iter is None:\n",
    "    best_iter = bst.num_boosted_rounds() - 1\n",
    "\n",
    "y_prob = bst.predict(dtest, iteration_range=(0, best_iter + 1))\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# --- 5) Package results (and save if you like) ---\n",
    "out = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"y_prob\": y_prob,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "print(out.head())\n",
    "\n",
    "# Optional: save submission/predictions\n",
    "out.to_csv(\"xgb_predictions.csv\", index=False)\n",
    "print(\"Saved predictions to xgb_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
